interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.32.5
    method: GET
    uri: https://api.rogue-scholar.org/posts/10.54900/r8zwg-62003
  response:
    body:
      string: '{"abstract":"Academia is undergoing a rapid transformation characterized
        by exponential growth of scholarly outputs. This phenomenon, often termed
        the \"firehose problem,\" presents significant challenges for researchers,
        publishers, funders, policymakers, and institutions alike.","archive_url":"https://wayback.archive-it.org/22124/2025-05-05T10:39:18Z/https://upstream.force11.org/drinking-from-the-firehose/","authors":[{"contributor_roles":[],"family":"Marcum","given":"Christopher
        Steven","url":"https://orcid.org/0000-0002-0899-6143"}],"blog":{"archive_collection":22124,"archive_host":null,"archive_prefix":"https://wayback.archive-it.org/22124/20231105103706/","archive_timestamps":[20231105103706,20240505132151,20241105103657,20250505103918],"authors":null,"canonical_url":null,"category":"humanities","community_id":"aeaafcbb-94b5-477a-a89f-8fba5925e926","created_at":1673568000,"current_feed_url":"https://upstream.force11.org/atom/","description":"The
        community blog for all things Open Research.","doi_as_guid":false,"favicon":"https://rogue-scholar.org/api/communities/b56ef314-34f7-4c7f-b0e2-d0bf13bfe83b/logo","feed_format":"application/atom+xml","feed_url":"https://upstream.force11.org/atom-complete/","filter":null,"funding":null,"generator":"Ghost","generator_raw":"Ghost
        5.25","home_page_url":"https://upstream.force11.org","id":"e3952730-ffb7-4ef9-b4a5-6433d86b2819","indexed":false,"issn":null,"language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","mastodon":"https://scicomm.xyz/@upstream","prefix":"10.54900","registered_at":1729244339,"relative_url":null,"ror":null,"secure":true,"slug":"upstream","status":"active","subfield":"1802","title":"Upstream","updated_at":1761638979,"use_api":true,"use_mastodon":false,"user_id":"08014cf6-3335-4588-96f4-c77ac1e535b2"},"blog_name":"Upstream","blog_slug":"upstream","citations":[{"citation":"https://doi.org/10.53731/2j1hr-5xc16","published_at":"2025-01-22","unstructured":"Fenner,
        M. (2025, January 22). Formatting references in blog posts. <i>Front Matter</i>.
        https://doi.org/10.53731/2j1hr-5xc16","updated_at":"2025-02-02T19:19:54.238037+00:00","validated":true},{"citation":"https://doi.org/10.53731/4cjmp-ky907","published_at":"2025-01-22","unstructured":"Fenner,
        M. (2025, January 22). Formatting references in blog posts. <i>Front Matter</i>.
        https://doi.org/10.53731/4cjmp-ky907","updated_at":"2025-10-22T07:56:53.278729+00:00","validated":true},{"citation":"https://doi.org/10.54900/f54sh-45820","published_at":"2025-07-22","unstructured":"Marcum,
        C. S. (2025, July 22). Capping APCs May Backfire on NIH. <i>Upstream</i>.
        https://doi.org/10.54900/f54sh-45820","updated_at":"2025-10-22T08:10:09.406196+00:00","validated":true},{"citation":"https://doi.org/10.54900/j3csy-hey68","published_at":"2025-07-22","unstructured":"Marcum,
        C. S. (2025, July 22). Capping APCs May Backfire on NIH. <i>Upstream</i>.
        https://doi.org/10.54900/j3csy-hey68","updated_at":"2025-10-22T07:52:06.188766+00:00","validated":true}],"content_html":"<figure
        class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://upstream.force11.org/content/images/2024/08/1000008255.jpg\"
        class=\"kg-image\" alt=\"alt_text\" loading=\"lazy\" title=\"image_tooltip\"
        width=\"1024\" height=\"1024\" srcset=\"https://upstream.force11.org/content/images/size/w600/2024/08/1000008255.jpg
        600w, https://upstream.force11.org/content/images/size/w1000/2024/08/1000008255.jpg
        1000w, https://upstream.force11.org/content/images/2024/08/1000008255.jpg
        1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space:
        pre-wrap;\">AI generated illustration using Microsoft Designer</span></figcaption></figure><p>Academia
        is undergoing a rapid transformation characterized by exponential growth of
        scholarly outputs. This phenomenon, often termed the \"firehose problem,\"
        presents significant challenges for researchers, publishers, funders, policymakers,
        and institutions alike. Some of these challenges include stresses on the peer-review
        system, a lower capacity to stay abreast of the latest research, a shift in
        the value of quantity over quality of scholarship, and a divergence between
        the rewards and incentives for producing outputs that meet funder and societal
        expectations. In this essay, the implications of the firehose problem and
        potential approaches to resolving it through reform of incentives and rewards
        for publishing are explored.</p><h3 id=\"the-firehose\">The firehose</h3><p>At
        the 2024 National Academies Workshop,<a href=\"https://www.nationalacademies.org/our-work/enhancing-public-access-to-the-results-of-research-supported-by-the-department-of-health-and-human-services-a-workshop\">
        Enhancing Public Access to the Results of Research Supported by the Department
        of Health and Human Services</a> Tom Ciavarella of Frontiers re-raised an
        idea that periodically pops up in conversations about scholarly publishing:
        there are too many research articles being submitted for publication to journals
        - that is, we publish too much. This was in response to a researcher who complained
        about the <a href=\"https://doi.org/10.1242/dmm.002758\" rel=\"noreferrer\">firehose
        problem</a> in academic publishing - it\u2019s challenging <a href=\"https://doi.org/10.48550/ARXIV.2309.15884\"
        rel=\"noreferrer\">to keep up with the volume</a> at which research publications
        are produced. The researcher was, by implication, placing some blame for the
        firehouse on the publishers. Ciavarella noted that publishers only respond
        to demand - they build bigger pipes on which to fit bigger hoses; if there
        is too much research coming out of the <a href=\"https://doi.org/10.1021/acsenergylett.4c01991\"
        rel=\"noreferrer\">publication firehose</a>, it\u2019s because there are too
        many submissions for publication, he claimed. The relationship between supply
        and demand in academic publishing is more complex than this analogy supposes.
        After all, the<a href=\"https://doi.org/10.4337/9781786434937.00007\" rel=\"noreferrer\">
        traditional currency</a> that academic institutions use in exchange for tenure
        and promotion for their faculty is peer-reviewed publications - and the fiat
        currency of publishers is peer-reviewed publications. More than fifty years
        after Silen\u2019s seminal editorial criticizing this currency, it\u2019s
        still a <a href=\"https://doi.org/10.1001/archsurg.1971.01350070027002\" rel=\"noreferrer\">publish
        or perish</a> world.</p><h3 id=\"upstream-of-the-nozzle\">Upstream of the
        nozzle</h3><p>To assess Ciavarella\u2019s claim, a reasonable estimate of
        the global number of article submissions to academic journals would be needed.
        Submissions represent the reservoir supplying the firehose. It is, however,
        incredibly difficult to get accurate data on the number of submissions to
        journals. Most publishers keep their data closed - a point that the Office
        of Science and Technology Policy respectfully glosses over in their Sisyphean
        <a href=\"https://www.whitehouse.gov/ostp/news-updates/reports-and-documents/\">reports
        to Congress</a> on public access to federally funded research. Publishers
        do, however, advertise their journal \u201cacceptance rates\u201d in part
        because of a belief that such rates are inversely proportional to journal
        prestige - the more submissions a journal rejects relative to the number they
        accept, the more prestigious that journal claims to be along with other factors
        such as readership, citation rates, etc. Publishers translate that <a href=\"https://blogs.lse.ac.uk/impactofsocialsciences/2024/07/09/designer-science-why-big-brand-journals-harm-research/\">prestige
        into brand power</a> - creating market forces that drive additional submissions
        from researchers eager to attach their names to recognizable brands.</p><p>In
        a preprint from a few years ago, Rachel Herbert of Elsevier\u2019s International
        Center for the Study of Research (ICSR), <a href=\"https://doi.org/10.2139/ssrn.3526365\"
        rel=\"noreferrer\">evaluated the acceptance rates</a> of over 2,000 journals
        (80% of them published by Elsevier) in 2017. The study found that the average
        acceptance rate was 32% with a range of 1.1% to 93.2% - this is similar to
        the rates found independently around the <a href=\"https://doi.org/10.3145/epi.2019.jul.07\">same
        time</a>. Acceptance rates, of course, are a function of editorial policy
        that can be influenced by the publisher through a range of factors such as
        issue page number allotments, scope of the journal field, editorial prerogative,
        reviewer consensus on potential scientific importance of a submission, and
        the number of submissions sent into the journal. Leaving aside the fact that
        acceptance rates can be artificially deflated (and prestige factors artificially
        inflated) by simply increasing the number of submissions of poor-quality manuscripts,
        they can be used in conjunction with estimates of the total number of articles
        published to come up with a reasonable back-of-the-envelope estimate of the
        number of submissions.</p><p>According to<a href=\"https://wordsrated.com/author/dimitrije-curcic/\">
        Dimitri Curcic at WordsRated</a>, since 1996 more than <a href=\"https://wordsrated.com/number-of-academic-papers-published-per-year/\">64
        million papers have been</a> published in academic journals and there has
        been a <a href=\"https://wordsrated.com/academic-publishing-statistics/\">23%
        increase</a> in growth in the last five years alone. During that time, the
        number of active journals has increased by more than 28% - outpacing the proliferation
        of articles (which suggests that the growth of these journals is somewhat
        indicative of having higher acceptance rates as there are more venues for
        articles that would have otherwise not been accepted for publications). These
        data also suggest that this relationship dampened in recent years, with the
        average annual growth rate of journals (1.67%, 2016-2020) being about a third
        of that of the number of published articles (5.28%, 2018-2022) in the last
        five years of available data for each.</p><p>The WordsRated data suggests
        that there were, conservatively, around 4 million articles published in 2017-the
        year of Herbert''s ICSR preprint (about <a href=\"https://ncses.nsf.gov/pubs/nsb20243/translation-u-s-and-global-science-technology-and-innovation-capabilities\">2.5
        million</a> of those were in science and engineering fields based on National
        Science Foundation estimates). Combining these data, we can reasonably estimate
        the range of articles submitted in 2017 at 12.5 million articles submitted
        globally - that''s almost 24 submissions per minute.<a><sup>[1]</sup></a>
        That is a lot of papers under consideration in the scholarly publishing market.</p><p>But
        is it too many submissions translating into too many papers coming out of
        the nozzle of the firehose? Because the firehose analogy is an oversimplification
        of a complex system we need to consider more factors than just the demand
        from researchers to have more outlets to publish in and the publishers\u2019
        response to supply those outlets. Whether this system has resulted in a surplus
        of published manuscripts \u2013 flooding the streets well after the fire is
        out \u2013 is a question about the quality of that surplus.</p><h3 id=\"is-the-water-from-the-firehose-safe-to-drink\">Is
        the water from the firehose safe to drink?</h3><p>Few would argue that a surplus
        of high-quality, groundbreaking, innovative research is characteristic of
        a system producing <em>too many papers</em>. On the other hand, an excess
        of poor quality, low impact, and questionable research published in journals
        that are fit for that purpose, should<a><sup>[2]</sup></a> give pause to anyone
        drinking from the firehose. That is, perhaps it\u2019s not just the excess
        number of papers that contributes to the surplus of frankly poor quality research
        published. Maybe there are too many poor quality journals too - responding
        to an underlying demand to publish low quality papers.</p><p>During the digital
        transformation to online publication, there was only one variable that the
        publishers needed to tune to satisfy this demand - increase the acceptance
        rate of existing journals. But doing that would have diminished the underlying
        value of their high prestige journals. So, instead, they tuned two variables
        by increasing the number of journals with higher acceptance rates in the system
        while protecting their big brands with constant rates. This is a practice
        that is often couched within the seemingly innocuous guise of field specialty
        journals. Why would field specialization need a lower threshold for publication
        if the merits of peer review are constant?</p><p>Here too, however, even after
        accounting for quality, the relationship between the number of submissions
        and the number of journals is more complex than a simple feedback loop. There
        is an underlying hidden dependence resulting from the very rational behavior
        of researchers contributing to this loop. The expansion of journals with higher
        acceptance rates alters the rational calculus for researchers - all things
        being equal higher acceptance rates create a perverse incentive to submit
        as many manuscripts as possible since the underlying probability of acceptance
        is simply higher than if those same publications were submitted to a journal
        with a lower acceptance rate, and hence higher prestige. Publishers often
        compound this incentive by offering automatic referrals for papers rejected
        from higher prestige journals to lower ones - allowing for almost instantaneous
        and friction-free forwarding of a rejected manuscript to <a href=\"https://pubs.acs.org/page/policy/manuscript_transfer/index.html\"
        rel=\"noreferrer\">another journal in their own portfolios </a>(and certainly
        never to a competitor''s journal). This feedback loop is self-replicating
        and self-expanding without significant disruption.</p><p>The idea that there
        are too many journals accepting too many manuscripts for publication is not
        new. Christa Easton <a href=\"https://doi.org/10.1080/00987913.1997.10764393\">summarized
        the pressures</a> that libraries faced sustaining subscriptions to the growing
        number of serial publications during the digital transformation in the 1990s
        while, simultaneously, publishers were faced with increased incentives to
        produce more journals. In 1997, the great migration to digital formats was
        only just beginning and came at significant risk and cost - now, it\u2019s
        effectively effortless for a publisher to spin off a new online-only journal.
        Publishers \u2013 and not just experienced publishers but really anybody including
        scholarly societies and unaffiliated individuals \u2013 can create a new journal
        in minutes. Researchers can \u2013 and do \u2013 respond to the availability
        by slicing up their work (and their data) into minimally publishable units
        (sometimes <a href=\"https://doi.org/10.1038/s41393-020-00543-y\" rel=\"noreferrer\">called
        salami slicing</a>) knowing that they have lower chances of contributing a
        single high-quality and holistic article to the scholarly record than they
        do contributing multiple smaller studies to a wider variety of field journals
        (which can contribute to <a href=\"https://doi.org/10.1016/j.jpsychores.2014.11.008\">self-citation
        and auto-plagiarism</a>).</p><h3 id=\"who-is-testing-the-water\">Who is testing
        the water?</h3><p>Another argument is that the growth in the number of journals
        and the number of published research articles has been <a href=\"https://doi.org/10.1128/iai.01530-13\"
        rel=\"noreferrer\">a response to scientific advances requiring increasingly
        specialized knowledge</a>. The scientific publication model, and the incentives
        superstructure supporting researcher participation in it, is based upon the
        premise that science advances by taking baby steps \u2013 little discoveries,
        a single hypothesis falsified, a new data point, a novel method, another patent.
        And yet, there is very little evidence that this slow march of science correlates
        positively with the volume of published research. In fact, there is a countervailing
        theory, and <a href=\"https://doi.org/10.4103/sja.SJA_544_18\" rel=\"noreferrer\">some
        evidence </a>to support it, that the overall quality of research is inversely
        proportional to the overall quantity of research or perhaps just a random
        consequence of the <a href=\"https://doi.org/10.1371/journal.pone.0178074\">pool
        of available sources to cite</a>.</p><p>Major breakthroughs, theories, and
        discoveries in science are rare and, relative to the volume of research over
        the last half-century, <a href=\"https://doi.org/10.1038/s41586-022-05543-x\"
        rel=\"noreferrer\">appear to be getting rarer</a>. If journals provide value
        proportional to their acceptance rates (i.e., a component of their prestige
        by the publishers'' metrics), then one might assume that the quality of reviews
        and rigor of the underlying research must also be of significantly higher
        quality than less prestigious journals, right? Well, if that''s the case,
        then must it also be true that a less prestigious journal has a poorer review
        system allowing for lower quality research to be published? If publishers
        were building bigger pipes solely in response to the increase in the volume
        of submissions flowing through the firehose to altruistically advance science
        in proportion to its progress \u2013 and not to increase their revenues \u2013
        they would have produced more journals with lower acceptance rates or improved
        the fittings and bushings connecting their pipes to the firehose by innovating
        with alternative business models (a few ideas that some good faith publishers
        have attempted to introduce into the system without much success in disruption
        include models such as compensating reviewers, charging non-refundable submission
        fees at the time of submission, etc). Two widespread and successful, if inequitable,
        innovations to come out of the <a href=\"https://www.doi.org/10.17705/1jais.00873\">digital
        transformation </a>in the last 20 years include the invention of article processing
        charges (APCs) and journal impact factors (JIFs<a><sup>[3]</sup></a>). APCs
        support the surplus revenues of the journals and not the quality of the underlying
        research that they publish. This latter point was highlighted in the <a href=\"https://www.budapestopenaccessinitiative.org/boai20/\">2022
        Budapest Open Access Initiative </a>20th anniversary statement: \"[APCs] don\u2019t
        pay for improved quality but for the perception of improved quality\u2026Career
        advancement can depend on that perception. But that is a problem to solve,
        not an immutable reality to accommodate.\u201d The JIFs were also called out
        by the statement as erroneously mistaking impact for <a href=\"https://doi.org/10.7554/eLife.47338\"
        rel=\"noreferrer\">quality and conflating journal with article impact.</a>
        Even if the JIFs and their individual impact factor counterparts were once
        an effective measure of <em>quality</em>, they are unlikely to<a href=\"https://doi.org/10.1093/gigascience/giz053\">
        remain effective as measures in general.</a></p><p>If peer reviewers have
        a role in the firehose analogy, <a href=\"https://doi.org/10.3389/bjbs.2024.12054\"
        rel=\"noreferrer\">they are supposed to play the part of water quality control</a>.
        However, across the academic publishing landscape, there is a mix of quality
        control through the peer review system \u2013 a mix that may be increasingly
        unreliable. It might seem intuitive to assume that, given the greater resources
        and credibility of higher prestige journals there''s a higher concentration
        of high quality rigorous reviews while the concentration of poor quality reviews
        is likely to pool in less prestigious outlets, but the <a href=\"https://doi.org/10.3389/fnhum.2018.00037\"
        rel=\"noreferrer\">evidence </a>appears to be <a href=\"https://doi.org/10.3389/fnhum.2013.00291\"
        rel=\"noreferrer\">to the contrary</a>. Of course, the worst of the worst
        falls within the <a href=\"https://doi.org/10.1177/0192623320920209\" rel=\"noreferrer\">predatory
        space</a> where there''s a complete absence of review or disingenuous promises
        of quality rapid review. Sometimes this takes the form of otherwise legitimate
        journals allowing for <a href=\"https://retractionwatch.com/2024/07/01/we-authors-paid-a-heavy-price-journal-retracts-all-23-articles-in-special-issue/\">greater
        tolerances to review processes</a> to accelerate the publication of \u201c<a
        href=\"https://origineditorial.com/original-thoughts-blog/the-perils-and-pitfalls-of-special-issues-and-how-to-avoid-them/\">special
        issues</a>\u201d (which sometimes come with the condition that <a href=\"https://doi.org/10.1002/leap.1406\">authors
        have to pay APCs</a> when they''re being invited by the journal to contribute
        - a suspect practice altogether). Certainly, no peer review system or peer
        reviewer pool is perfect and there are significant and often consequential
        lapses in scientific integrity or methodological concerns that escape even
        the most rigorous peer review and end up in a prestigious publication outlet.</p><p>The
        most egregious and<a href=\"https://www.vice.com/en/article/4a389b/ai-midjourney-rat-penis-study-retracted-frontiers\">
        sometimes hilarious</a> lapses in the peer review system in high profile journals
        \u2013 <a href=\"https://www.nature.com/articles/s41586-023-05742-0\">some
        at the very apex of prestige</a> \u2013 only demonstrate that there is a significant
        problem with peer review at a systemic level.</p><p>The conjecture advanced
        here, <a href=\"https://scholarlykitchen.sspnet.org/2021/10/12/recognition-in-peer-review/\">and
        by others</a>, is that this problem is threefold:</p><ol><li>The demand for
        peer review is too high;</li><li>The rewards for conducting peer review are
        too low;</li><li>The resources required for rigorous peer review are insufficient
        for most peer review editorial systems.</li></ol><p>The initial conditions
        of the first aspect of the peer review problem have already been implied here:
        the likelihood that there are too many papers and too many journals for the
        system to support. If demand for peer review is too high, it\u2019s likely
        because of a deficit in the supply of qualified peers available to conduct
        review.</p><p>There''s good data from the National Science Foundation to support
        that claim. According to the <a href=\"https://ncses.nsf.gov/pubs/nsf23300/report\">latest
        report</a> on the results of the Survey of Earned Doctorates by the NSF\u2019s
        National Center for Science and Engineering Statistics, the number of earned
        doctorates in science and engineering fields has risen on average over time.
        The trend peaked in 2019 at 42,898 doctorates awarded before declining slightly<a
        href=\"https://ncses.nsf.gov/pubs/nsf23300/report/special-focus-covid-19-pandemic-impacts-on-doctorate-recipients\">
        during the pandemic years.</a></p><p>However, even as universities are awarding
        a greater number of doctorates over time, fewer and fewer are entering into
        academic jobs at universities where the vast majority of journals draw their
        editorial review boards. At the<a href=\"https://www.nationalacademies.org/event/41687_06-2024_the-state-of-the-science\">
        2024 State of the Science Address</a>, NASEM President Marcia McNutt called
        attention to the deficit in academic replacement. Just 36% of doctorate recipients
        not immediately going into a post-doctoral fellowship in 2021 reported that
        their first job out of graduate school would be in academia. That\u2019s a
        significant decline from 48% of such in 2001. Now, it\u2019s true that a greater
        number of doctorate recipients are entering into postdoctoral fellowships
        but the rate hasn\u2019t kept up with the decline in academe overall \u2013
        postdoctoral fellowships are not offsetting the fewer career positions filled
        in academia. Almost everyone getting a doctorate goes into a non-university
        position after graduation and for good reason: the academy cannot compete
        with the salary, benefits, and job security and stability that other sectors
        provide to newly minted PhDs. And when they get into those jobs, very few
        PhDs continue to publish in or review for scholarly outlets at high rates
        \u2013 some journals have strict review policies that do not allow non-academics
        or individuals who have not published recently to participate in peer review,
        which compounds the problem.</p><p>For those that remain in the academy, participation
        in peer review \u2013 ostensibly lauded as a critical component of the scientific
        process \u2013 is not given adequate recognition and reward by academic institutions.
        Peer review is often treated as <a href=\"https://doi.org/10.12688/f1000research.16493.1\"
        rel=\"noreferrer\"><em>service</em></a> by tenure and promotion committees,
        which is the least important aspect of the expected contributions of scholars.
        Recognition mechanisms outside of the academy, and metadata infrastructure
        to support them, is <a href=\"https://scholarlykitchen.sspnet.org/2021/10/12/recognition-in-peer-review/\">increasingly
        becoming available</a><a><sup>[4]</sup></a> and yet the culture of the academy
        has been slow to adopt these innovations.</p><p>Based on these trends, it\u2019s
        very likely that the proliferation in the number of journals and articles
        rose concurrently with a decline in the availability of qualified reviewers
        to conduct robust, high quality peer review over the same time period. Conveniently,
        it appears that not only is there a demand to publish low quality research,
        there is a deficit in the quality of the peer review system overall anyway.
        Here too, artificial intelligence \u2013 even with its <a href=\"https://www.doi.org/10.1001/jama.2023.12500\">potential
        benefits </a>to the peer review system (such as augmented literature search,
        detection of falsified data and images, and translation services) \u2013 is
        likely contributing to a downturn in thoughtful and rigorous review as generative
        AI can both do the reading and the writing on behalf of a human reviewer.
        Some publishers require reviewers to<a href=\"https://editorresources.taylorandfrancis.com/reviewer-guidelines/\">
        abstain from using those tools</a>, even when they could be helpful. We can
        be certain that, given the unrewarded high demands on qualified human reviewers,
        they are indeed employing this tactic without disclosure.</p><p>All of this
        belies a significant crisis <a href=\"https://doi.org/10.1111/hequ.12511\"
        rel=\"noreferrer\">unfolding within the scholarly peer review system</a><a><sup>[5]</sup></a>.
        Legitimate journals are finding it increasingly difficult to solicit and retain
        high quality and timely responses to invitations for peer review. The pressures
        are perverse for potential reviewers. The motivations for contributing to
        peer review are almost entirely altruistic, save for the ability to stay abreast
        of potentially competing research \u2013 or, dreadfully, quell competing research
        from behind the veil of anonymity. There''s very little incentive provided
        by publishers, academic institutions, and scholarly societies for potential
        reviewers to contribute their valuable time in an increasingly productivity-constrained
        environment. For many journals, the time between the initial invitation and
        final decision is substantially delayed and editorial boards have to reach
        an increasing number of potential reviewers before finding someone available
        and willing to do reviews. Worse, there is very little done by editorial boards
        or journals <a href=\"https://www.doi.org/10.1186/s12874-019-0688-x\">to assess
        and report on the quality of reviews</a> and to incentivize potential reviewers
        to maintain the scientific integrity of the reviews they provide. Authors
        are required to disclose conflicts of interest, reviewers however, rarely
        if ever are expected to do the same.</p><p>Still, peer review remains an <a
        href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4975196/\">incredibly
        valuable asset to the research enterprise</a>. When done effectively and transparently,
        peer review provides significant benefits to science, including: improving
        the quality and rigor of works, grammatical and typesetting corrections, identifying
        theoretical gaps and insights, fostering collaborations, conferring public
        trust in the oversight of science, and more. But this crisis in peer review
        threatens all of its benefits. Without intervention and significant reform
        in the culture of scholarly publishing, it will be peer review \u2013 and
        not unsustainable publication business models \u2013 that undermines science
        the most. The firehose will continue to flow even as the water quality tests
        fail to pass potability standards.</p><h3 id=\"sip-from-the-spring\">Sip from
        the spring</h3><p>Silen, <a href=\"https://doi.org/10.1021/acsenergylett.4c01991\"
        rel=\"noreferrer\">and many others after</a>, point out the unintended consequences
        the publish or perish culture in academia has created for the research community
        that have been revisited here: a proliferation of low-quality publications,
        slow review times-or false-promises for high-quality fast review-proliferation
        of predatory journals and paper mills, and more. With the recent emergence
        and widespread availability of generative artificial intelligence, all of
        these problems are accelerating in magnitude and velocity and will continue
        to put significant strain on the fittings and controls in this system \u2013
        possibly until the hose bursts. As we enter a<a href=\"https://doi.org/10.18665/sr.320210\">
        second digital transformation</a> in scholarly writing, one characterized
        by demand for data and code sharing in publicly accessible repositories and
        the challenges and promises of AI<a><sup>[6]</sup></a>, reform in infrastructure
        alone will be insufficient to support the forthcoming deluge \u2013 we need
        significant changes to the incentive structure for academic scholarship to
        lower the pressure, and increase the quality, of the water coming out of the
        firehose.</p><p>Researchers hold an incredible amount of market power in scholarly
        publishing - they drive both the supply of and demand for manuscripts. Researchers
        can, and should, leverage that power to challenge the status-quo and resolve
        the firehouse problem <em>that they themselves decry</em>. They are the source
        of the <a href=\"https://en.wikipedia.org/wiki/Pierian_Spring\">Pieran spring</a>
        and everything downstream depends on its flow. One way to temper the firehouse
        would be to sip directly from the spring rather than from the nozzle.</p><p>Scholarly
        writing is much richer than just publications. For example, researchers produce
        grant proposals, editorials, policy briefs, blog posts, teaching curricula
        and lectures, software code and documentation, dataset curation, and labnotes
        and codebooks. Some of these scholarly outputs may end up being published
        \u2013 some may even end up <a href=\"https://jupyter.org/\">changing how
        science is communicated and conducted.</a> But, realistically, most will not
        obtain the recognition that the authors and contributions deserve for these
        <a href=\"https://doi.org/10.7551/mitpress/12200.003.0017\" rel=\"noreferrer\">\u201cnon-traditional\u201d
        outputs</a>. Much of these outputs hold incredible value to the scientific
        community and to the public. A singular focus on writing manuscripts to submit
        for publication lowers the likelihood that the value of these other materials
        can be realized.</p><p>By writing more and publishing less, researchers can
        lower the pressure of the firehose while continuing to make valuable contributions
        to the world. When academics write policy briefs that inform legislation,
        create a blog to enhance dialogue in their field, produce open data that is
        broadly reused, or write open software that enables a broad gold standard
        method to be widely available, they should be rewarded with value on par with
        any particular peer-reviewed publication in a journal. All of these materials
        fill the spring even if only published manuscripts filter through to the firehose.</p><p>The
        superstructure of incentives \u2013 predominantly those that provide credit
        for the purposes of tenure, promotion, and other career advances \u2013 should
        treat some combination of these outputs with parity to publications. Increasingly,
        <a href=\"https://asapbio.org/funder-policies\">funder policies require preprints</a>,
        <a href=\"https://sharing.nih.gov/data-management-and-sharing-policy\">data
        </a>and <a href=\"https://www.nasa.gov/wp-content/uploads/2021/12/nasa-ocs-public-access-plan-may-2023.pdf\">code
        sharing</a>, and other research outputs that they support with their grant
        money: some funders incentivize these requirements by rewarding compliance
        with parity to publications for the purposes of future grant review \u2013
        the same should be true of home institutions that receive the funds that researchers
        are awarded and researchers should demand that the full cornucopia of the
        work supported by their grant is rewarded equitably for the purposes of performance,
        tenure, and promotion review. That is not to say that major breakthroughs
        and discoveries should not receive special attention \u2013 rather, it\u2019s
        a proposal to amplify the entire portfolio of work that led to those discoveries.</p><p>Sharing
        ideas earlier with the community can greatly improve the quality of scholarship
        and broaden the impact and reach of those ideas. One way to accomplish this
        with research is by contributing to preprints and preprint reviews. Preprints,
        of course, can be added to a repository without ever undergoing review - that\u2019s
        both an advantage and a disadvantage. On the one hand mass adoption of preprint
        use could shift the firehose problem to a problem of a poisoned spring with
        a large volume of unreviewed manuscripts filling the pool \u2013 on the other,<a
        href=\"https://aj-boston.pubpub.org/pub/mtrwtisx/release/3\"> preprints can
        be checked by many more potential reviewers</a> and provide an avenue for
        sharing important results that may not otherwise get published in a journal
        (such as null-results). To help with the balance, preprint review provides
        an additional filter from the spring into the firehouse.</p><p>Preprint review
        is increasingly important as a <a href=\"https://doi.org/10.1371/journal.pbio.3002502\">mechanism
        to reform the peer review system</a> \u2013 reform that\u2019s happening in
        a manner that seeks to shift peer review from a monoculture maintained solely
        by publishers into an entire ecosystem largely maintained by researchers,
        their institutions, and their funders. With preprint review, authors participate
        in a system that views peer review not as a gatekeeping hurdle to overcome
        to reach publication but as a participatory exercise to improve scholarship.</p><p>Preprint
        review can also reveal potential errors and issues of scientific integrity
        earlier in the development of a manuscript so that authors can make more informed
        decisions about the state of their research and what should be done to address
        those issues in future revisions. Also, because preprint review is done out
        in the open, reviewers have an opportunity to interact with one another and
        to expose disagreements, highlight consistencies, and respond to ideas that
        ultimately allow authors to have a more holistic review. It\u2019s also a
        way to ensure that authors can retain full control over their intellectual
        property and its derivatives by asserting licenses that fit with their personal
        needs and values (e.g., using a CC-BY-NC license if one does not want <a href=\"https://www.thebookseller.com/news/academic-authors-shocked-after-taylor--francis-sells-access-to-their-research-to-microsoft-ai\">publishers
        to sell the content of a manuscript for use in AI, for instance</a>).</p><p>The
        macro-effect of reforming peer review to include widespread use of preprint
        review would align nicely with a widely held philosophy of science that treats
        science <a href=\"https://doi.org/10.22323/2.20030401\">communication to be
        a conversation rather than a broadcast.</a> Models of preprint review are
        many, including relatively novel approaches such as <a href=\"https://prereview.org/\">PREreview</a>\u2019s
        live review where multiple reviewers collaborate to <a href=\"https://prereview.org/live-reviews\">review
        a preprint in real time online</a> or ASAPbio\u2019s <a href=\"https://asapbio.org/crowd-preprint-review\">crowd
        preprint</a> review done collaboratively buy asynchronously. Some journals,
        like eLife\u2019s new model, already consider <a href=\"https://elifesciences.org/inside-elife/66d43597/elife-s-new-model-one-year-on\">preprint
        review in their editorial pipelines</a> \u2013 bypassing the need to solicit
        additional feedback and accelerating editorial decisions. Recently, The Gates
        Foundation <a href=\"https://openaccess.gatesfoundation.org/\">refreshed their
        open access policy</a> to require deposit of preprints by their grantees.
        Certainly, funders like Gates continue to value peer review and greater adoption
        of preprint review can shift the inaccurate belief that all preprints lack
        review (a belief codified in Gates\u2019 required disclosure for researchers
        posting preprints).</p><p>The firehose problem in academic publishing is unlikely
        to be resolved by changing the pipe fittings alone. Of course, this is the
        rational option that publishers choose in response to the apparent demand
        from researchers - as Ciavarella rightly pointed out. The underlying demand,
        however, is fueled by a complex of misaligned and perverse incentives to publish
        or perish in academia. Reform at the fittings has only compounded the problem
        \u2013 increasing additional demand on an already stressed peer-review system
        and favoring quantity over quality of published manuscripts. A more holistic
        approach should focus repairs upstream, away from the nozzle, and to the source
        of knowledge that flows through the firehose itself \u2013 a spring filled
        with a variety of scholarly outputs beyond just manuscripts. Incentives for
        filling that spring should mesh with the rewards for publishing offered by
        funders, academic institutions, policymakers, publishers, and researchers
        themselves. This requires <a href=\"https://doi.org/10.1371/journal.pone.0265506\"
        rel=\"noreferrer\">collegiality</a> across and between all of those stakeholders
        to <a href=\"https://www.timeshighereducation.com/opinion/open-access-loses-when-publishers-are-vilified\">work
        together</a> without polarization. In sum, alleviating the pressure coming
        out of the firehose is straightforward when the incentives are appropriately
        and collaboratively aligned: write more and publish less.</p><h2 id=\"acknowledgments\">Acknowledgments</h2><p>Special
        thanks are owed to Stuart Buck, Tom Ciavarella, Erin McKiernan, Peter Suber,
        and Crystal Tristch for their efforts in improving this work.</p><h2 id=\"editorial-assessment\">Editorial
        Assessment</h2><p>Three reviewers <a href=\"https://metaror.org/kotahi/articles/40/index.html\"
        rel=\"noreferrer\">have reviewed</a> this blog post via the MetaROR platform:</p><blockquote>In
        this blog post the author argues that problematic incentive structures have
        led to a rapid increase in the publication of low-quality research articles
        and that stakeholders need to work together to reform incentive structures.
        The blog post has been reviewed by three reviewers. Reviewer 3 considers the
        blog post to be a \u2018great piece\u2019, and Reviewer 1 finds it \u2018compellingly
        written and thought provoking\u2019. According to Reviewer 2, the blog post
        does not offer significant new insights for readers already familiar with
        the topic. All three reviewers provide recommendations for clarifications.
        Reviewers 2 and 3 also suggest the blog post could be more critical toward
        publishers. Reviewers 1 and 3 suggest taking a broader perspective on incentives,
        for instance by also considering incentives related to teaching and admin
        or incentives for funders, libraries, and other organizations.</blockquote><h2
        id=\"disclosure\">Disclosure</h2><p>The opinions expressed here are my own
        and may not represent those of my employer, my position, or the reviewers.
        For full transparency: I am a member of the scientific advisory board of PREreview,
        which I cited here. I contributed - either as author or reviewer - to a few
        of the papers incorporated by reference above. I have been guest editor or
        associate editor on a number of special issues during my academic career though
        I have never participated in soliciting direct contributions to those issues.
        I have made every attempt at citing works that are publicly accessible - a
        few works may not be freely available to all readers.</p><h2 id=\"notes\">Notes</h2><ol><li>(4M
        publications / 0.32 acceptance rate) = 12.5M submissions; (12.5M submissions
        / 525,600 minutes in a year) = 24 submissions per minute. This should conservatively
        underestimate the true rate. <a>\u21a9\ufe0e</a></li><li>The same argument
        about the balance of quantity over quality has also been made <a href=\"https://lithub.com/there-are-too-many-books-or-publishing-shouldnt-be-all-about-quantity/\">about
        books too</a>. <a>\u21a9\ufe0e</a></li><li>Pronounced like, but not to be
        confused with, the famous <a href=\"https://www.jif.com/the-spread/jif-vs-gif\">brand
        of peanut butter.</a> <a>\u21a9\ufe0e</a></li><li>A trusted colleague once
        warned: \u201cbe careful what you eat in the scholarly kitchen.\u201d Thankfully,
        this article is good soup. <a>\u21a9\ufe0e</a></li><li>This is the most comprehensive
        and up-to-date review of peer review currently available. The scope of the
        article demonstrates the value of peer review, its novelty as 20th Century
        practice, and the challenges that jeopardize its contemporary legitimacy in
        the 21st Century. It\u2019s well worth a read and it is available open access
        for free. There is another recent great article about the crisis in peer-review
        by Colleen Flaherty <a href=\"https://www.insidehighered.com/news/2022/06/13/peer-review-crisis-creates-problems-journals-and-scholars\">behind
        a paywall here</a>. <a>\u21a9\ufe0e</a></li><li>There are new <a href=\"https://www.science.org/content/article/how-keep-scientific-literature\">tools</a>
        available to researchers for writing and conducting peer review, including
        emerging <a href=\"https://scite.ai/\">artificial intelligence tools</a>.
        <a>\u21a9\ufe0e</a></li></ol><h2 id=\"references\">References</h2><ol><li>Patil,
        C., &amp; Siegel, V. (2009). Drinking from the firehose of scientific publishing.
        <em>Disease Models &amp; Mechanisms</em>, <em>2</em>(3\u20134), 100\u2013102.
        <a href=\"https://doi.org/10.1242/dmm.002758\">https://doi.org/10.1242/dmm.002758</a></li><li>Hanson,
        M. A., Barreiro, P. G., Crosetto, P., &amp; Brockington, D. (2023). <em>The
        strain on scientific publishing</em> (Version 2). arXiv. <a href=\"https://doi.org/10.48550/arxiv.2309.15884\"
        rel=\"noreferrer\">https://doi.org/10.48550/arxiv.2309.15884</a></li><li>Jin,
        S. (2024). Should We Publish Fewer Papers? <em>ACS Energy Letters</em>, <em>9</em>(8),
        4196\u20134198. <a href=\"https://doi.org/10.1021/acsenergylett.4c01991\">https://doi.org/10.1021/acsenergylett.4c01991</a></li><li>Publish
        or perish: Origin and perceived benefits. (2018). In I. Moosa, <em>Publish
        or Perish</em> (pp. 1\u201317). Edward Elgar Publishing. <a href=\"https://doi.org/10.4337/9781786434937.00007\">https://doi.org/10.4337/9781786434937.00007</a></li><li>Silen,
        W. (1971). Publish or Perish. <em>Archives of Surgery</em>, <em>103</em>(1),
        1. <a href=\"https://doi.org/10.1001/archsurg.1971.01350070027002\">https://doi.org/10.1001/archsurg.1971.01350070027002</a></li><li>Herbert,
        R. (2020). <em>Accept Me, Accept Me Not: What Do Journal Acceptance Rates
        Really Mean? [ICSR Perspectives]</em>. <a href=\"https://doi.org/10.2139/ssrn.3526365\">https://doi.org/10.2139/ssrn.3526365</a></li><li>Bj\u00f3rk,
        B.-C. (2019). Acceptance rates of scholarly peer-reviewed journals: A literature
        survey. <em>El Profesional de La Informaci\u00f3n</em>, <em>28</em>(4). <a
        href=\"https://doi.org/10.3145/epi.2019.jul.07\">https://doi.org/10.3145/epi.2019.jul.07</a></li><li>Easton,
        C. (1997). Too many journals, in too many forms? <em>Serials Review</em>,
        <em>23</em>(3), 64\u201368. <a href=\"https://doi.org/10.1080/00987913.1997.10764393\">https://doi.org/10.1080/00987913.1997.10764393</a></li><li>Harvey,
        L. A. (2020). We need to value research quality more than quantity. <em>Spinal
        Cord</em>, <em>58</em>(10), 1047\u20131047. <a href=\"https://doi.org/10.1038/s41393-020-00543-y\">https://doi.org/10.1038/s41393-020-00543-y</a></li><li>Ioannidis,
        J. P. A. (2015). A generalized view of self-citation: Direct, co-author, collaborative,
        and coercive induced self-citation. <em>Journal of Psychosomatic Research</em>,
        <em>78</em>(1), 7\u201311. <a href=\"https://doi.org/10.1016/j.jpsychores.2014.11.008\">https://doi.org/10.1016/j.jpsychores.2014.11.008</a></li><li>Casadevall,
        A., &amp; Fang, F. C. (2014). Specialized Science. <em>Infection and Immunity</em>,
        <em>82</em>(4), 1355\u20131360. <a href=\"https://doi.org/10.1128/iai.01530-13\"
        rel=\"noreferrer\">https://doi.org/10.1128/iai.01530-13</a></li><li>Tumin,
        D., &amp; Tobias, J. (2019). The peer review process. <em>Saudi Journal of
        Anaesthesia</em>, <em>13</em>(5), 52. <a href=\"https://doi.org/10.4103/sja.SJA_544_18\">https://doi.org/10.4103/sja.SJA_544_18</a></li><li>Michalska-Smith,
        M. J., &amp; Allesina, S. (2017). And, not or: Quality, quantity in scientific
        publishing. <em>PLOS ONE</em>, <em>12</em>(6), e0178074. <a href=\"https://doi.org/10.1371/journal.pone.0178074\">https://doi.org/10.1371/journal.pone.0178074</a></li><li>Park,
        M., Leahey, E., &amp; Funk, R. J. (2023). Papers and patents are becoming
        less disruptive over time. <em>Nature</em>, <em>613</em>(7942), 138\u2013144.
        <a href=\"https://doi.org/10.1038/s41586-022-05543-x\">https://doi.org/10.1038/s41586-022-05543-x</a></li><li>Avital,
        M. &amp; Copenhagen Business School. (2024). Digital Transformation of Academic
        Publishing: A Call for the Decentralization and Democratization of Academic
        Journals. <em>Journal of the Association for Information Systems</em>, <em>25</em>(1),
        172\u2013181. <a href=\"https://doi.org/10.17705/1jais.00873\">https://doi.org/10.17705/1jais.00873</a></li><li>McKiernan,
        E. C., Schimanski, L. A., Mu\u00f1oz Nieves, C., Matthias, L., Niles, M. T.,
        &amp; Alperin, J. P. (2019). Use of the Journal Impact Factor in academic
        review, promotion, and tenure evaluations. <em>eLife</em>, <em>8</em>, e47338.
        <a href=\"https://doi.org/10.7554/eLife.47338\">https://doi.org/10.7554/eLife.47338</a></li><li>Fire,
        M., &amp; Guestrin, C. (2019). Over-optimization of academic publishing metrics:
        Observing Goodhart\u2019s Law in action. <em>GigaScience</em>, <em>8</em>(6),
        giz053. <a href=\"https://doi.org/10.1093/gigascience/giz053\">https://doi.org/10.1093/gigascience/giz053</a></li><li>Drozdz,
        J. A., &amp; Ladomery, M. R. (2024). The Peer Review Process: Past, Present,
        and Future. <em>British Journal of Biomedical Science</em>, <em>81</em>, 12054.
        <a href=\"https://doi.org/10.3389/bjbs.2024.12054\">https://doi.org/10.3389/bjbs.2024.12054</a></li><li>Brembs,
        B. (2018). Prestigious Science Journals Struggle to Reach Even Average Reliability.
        <em>Frontiers in Human Neuroscience</em>, <em>12</em>, 37. <a href=\"https://doi.org/10.3389/fnhum.2018.00037\">https://doi.org/10.3389/fnhum.2018.00037</a></li><li>Brembs,
        B., Button, K., &amp; Munaf\u00f2, M. (2013). Deep impact: Unintended consequences
        of journal rank. <em>Frontiers in Human Neuroscience</em>, <em>7</em>. <a
        href=\"https://doi.org/10.3389/fnhum.2013.00291\">https://doi.org/10.3389/fnhum.2013.00291</a></li><li>Elmore,
        S. A., &amp; Weston, E. H. (2020). Predatory Journals: What They Are and How
        to Avoid Them. <em>Toxicologic Pathology</em>, <em>48</em>(4), 607\u2013610.
        <a href=\"https://doi.org/10.1177/0192623320920209\">https://doi.org/10.1177/0192623320920209</a></li><li>Repiso,
        R., Segarra\u2010Saavedra, J., Hidalgo\u2010Mar\u00ed, T., &amp; Tur\u2010Vi\u00f1es,
        V. (2021). The prevalence and impact of special issues in communications journals
        2015\u20132019. <em>Learned Publishing</em>, <em>34</em>(4), 593\u2013601.
        <a href=\"https://doi.org/10.1002/leap.1406\">https://doi.org/10.1002/leap.1406</a></li><li>Schimanski,
        L. A., &amp; Alperin, J. P. (2018). The evaluation of scholarship in academic
        promotion and tenure processes: Past, present, and future. <em>F1000Research</em>,
        <em>7</em>, 1605. <a href=\"https://doi.org/10.12688/f1000research.16493.1\">https://doi.org/10.12688/f1000research.16493.1</a></li><li>Flanagin,
        A., Kendall-Taylor, J., &amp; Bibbins-Domingo, K. (2023). Guidance for Authors,
        Peer Reviewers, and Editors on Use of AI, Language Models, and Chatbots. <em>JAMA</em>,
        <em>330</em>(8), 702. <a href=\"https://doi.org/10.1001/jama.2023.12500\">https://doi.org/10.1001/jama.2023.12500</a></li><li>Horta,
        H., &amp; Jung, J. (2024). The crisis of peer review: Part of the evolution
        of science. <em>Higher Education Quarterly</em>, e12511. <a href=\"https://doi.org/10.1111/hequ.12511\">https://doi.org/10.1111/hequ.12511</a></li><li>Superchi,
        C., Gonz\u00e1lez, J. A., Sol\u00e0, I., Cobo, E., Hren, D., &amp; Boutron,
        I. (2019). Tools used to assess the quality of peer review reports: A methodological
        systematic review. <em>BMC Medical Research Methodology</em>, <em>19</em>(1),
        48. <a href=\"https://doi.org/10.1186/s12874-019-0688-x\">https://doi.org/10.1186/s12874-019-0688-x</a></li><li>Bergstrom,
        T., Rieger, O. Y., &amp; Schonfeld, R. C. (2024). <em>The Second Digital Transformation
        of Scholarly Publishing: Strategic Context and Shared Infrastructure</em>.
        Ithaka S+R. <a href=\"https://doi.org/10.18665/sr.320210\">https://doi.org/10.18665/sr.320210</a></li><li>Alperin,
        J. P., Schimanski, L. A., La, M., Niles, M. T., &amp; McKiernan, E. C. (2022).
        The Value of Data and Other Non-traditional Scholarly Outputs in Academic
        Review, Promotion, and Tenure in Canada and the United States. In A. L. Berez-Kroeker,
        B. McDonnell, E. Koller, &amp; L. B. Collister (Eds.), <em>The Open Handbook
        of Linguistic Data Management</em> (pp. 171\u2013182). The MIT Press. <a href=\"https://doi.org/10.7551/mitpress/12200.003.0017\">https://doi.org/10.7551/mitpress/12200.003.0017</a></li><li>Avissar-Whiting,
        M., Belliard, F., Bertozzi, S. M., Brand, A., Brown, K., Cl\u00e9ment-Stoneham,
        G., Dawson, S., Dey, G., Ecer, D., Edmunds, S. C., Farley, A., Fischer, T.
        D., Franko, M., Fraser, J. S., Funk, K., Ganier, C., Harrison, M., Hatch,
        A., Hazlett, H., \u2026 Williams, M. (2024). Recommendations for accelerating
        open preprint peer review to improve the culture of science. <em>PLOS Biology</em>,
        <em>22</em>(2), e3002502. <a href=\"https://doi.org/10.1371/journal.pbio.3002502\">https://doi.org/10.1371/journal.pbio.3002502</a></li><li>Bucchi,
        M., &amp; Trench, B. (2021). Rethinking science communication as the social
        conversation around science. <em>Journal of Science Communication</em>, <em>20</em>(03),
        Y01. <a href=\"https://doi.org/10.22323/2.20030401\">https://doi.org/10.22323/2.20030401</a></li><li>Dawson,
        D. (DeDe), Morales, E., McKiernan, E. C., Schimanski, L. A., Niles, M. T.,
        &amp; Alperin, J. P. (2022). The role of collegiality in academic review,
        promotion, and tenure. <em>PLOS ONE</em>, <em>17</em>(4), e0265506. <a href=\"https://doi.org/10.1371/journal.pone.0265506\">https://doi.org/10.1371/journal.pone.0265506</a></li></ol>","doi":"https://doi.org/10.54900/r8zwg-62003","funding_references":null,"guid":"https://doi.org/10.54900/r8zwg-62003","id":"f3019896-187f-4e9f-8d76-df4fd1c47215","image":null,"indexed_at":1764088880,"language":"en","parent_doi":null,"published_at":1724763611,"reference":[{"id":"https://doi.org/10.1242/dmm.002758","type":"JournalArticle","unstructured":"Patil,
        C., &amp; Siegel, V. (2009). Drinking from the firehose of scientific publishing.
        <i>Disease Models & Mechanisms</i>, <i>2</i>(3-4), 100\u2013102. https://doi.org/10.1242/dmm.002758"},{"id":"https://doi.org/10.48550/arxiv.2309.15884","type":"Document","unstructured":"Hanson,
        M. A., Barreiro, P. G., Crosetto, P., &amp; Brockington, D. (2023). <i>The
        strain on scientific publishing</i> (Version 2). arXiv. https://doi.org/10.48550/arxiv.2309.15884"},{"id":"https://doi.org/10.1021/acsenergylett.4c01991","type":"JournalArticle","unstructured":"Jin,
        S. (2024). Should We Publish Fewer Papers?. <i>ACS Energy Letters</i>, <i>9</i>(8),
        4196\u20134198. https://doi.org/10.1021/acsenergylett.4c01991"},{"id":"https://doi.org/10.4337/9781786434937.00007","unstructured":"Publish
        or perish: Origin and perceived benefits. (2018). In I. Moosa, Publish or
        Perish (pp. 1\u201317). Edward Elgar Publishing. https://doi.org/10.4337/9781786434937.00007"},{"id":"https://doi.org/10.1001/archsurg.1971.01350070027002","type":"JournalArticle","unstructured":"SILEN,
        W. (1971). Publish or Perish. <i>Archives of Surgery</i>, <i>103</i>(1), 1.
        https://doi.org/10.1001/archsurg.1971.01350070027002"},{"id":"https://doi.org/10.2139/ssrn.3526365","type":"Article","unstructured":"Herbert,
        R. (2020). Accept Me, Accept Me Not: What Do Journal Acceptance Rates Really
        Mean? [ICSR Perspectives]. In <i>SSRN</i>. Elsevier BV. https://doi.org/10.2139/ssrn.3526365"},{"id":"https://doi.org/10.3145/epi.2019.jul.07","type":"JournalArticle","unstructured":"Bj\u00f3rk,
        B.-C. (2019). Acceptance rates of scholarly peer-reviewed journals: A literature
        survey. <i>El Profesional De La Informaci\u00f3n</i>, <i>28</i>(4). https://doi.org/10.3145/epi.2019.jul.07"},{"id":"https://doi.org/10.1080/00987913.1997.10764393","type":"JournalArticle","unstructured":"Easton,
        C. (1997). Too many journals, in too many forms?. <i>Serials Review</i>, <i>23</i>(3),
        64\u201368. https://doi.org/10.1080/00987913.1997.10764393"},{"id":"https://doi.org/10.1038/s41393-020-00543-y","type":"JournalArticle","unstructured":"Harvey,
        L. A. (2020). We need to value research quality more than quantity. <i>Spinal
        Cord</i>, <i>58</i>(10), 1047\u20131047. https://doi.org/10.1038/s41393-020-00543-y"},{"id":"https://doi.org/10.1016/j.jpsychores.2014.11.008","type":"JournalArticle","unstructured":"Ioannidis,
        J. P. A. (2015). A generalized view of self-citation: Direct, co-author, collaborative,
        and coercive induced self-citation. <i>Journal of Psychosomatic Research</i>,
        <i>78</i>(1), 7\u201311. https://doi.org/10.1016/j.jpsychores.2014.11.008"},{"id":"https://doi.org/10.1128/iai.01530-13","type":"JournalArticle","unstructured":"Casadevall,
        A., Fang, F. C., &amp; Morrison, R. P. (2014). Specialized Science. <i>Infection
        and Immunity</i>, <i>82</i>(4), 1355\u20131360. https://doi.org/10.1128/iai.01530-13"},{"id":"https://doi.org/10.4103/sja.sja_544_18","type":"JournalArticle","unstructured":"Tobias,
        J., &amp; Tumin, D. (2019). The peer review process. <i>Saudi Journal of Anaesthesia</i>,
        <i>13</i>(5), 52. https://doi.org/10.4103/sja.sja_544_18"},{"id":"https://doi.org/10.1371/journal.pone.0178074","type":"JournalArticle","unstructured":"Michalska-Smith,
        M. J., Allesina, S., &amp; Jadhao, S. B. (2017). And, not or: Quality, quantity
        in scientific publishing. <i>PLOS ONE</i>, <i>12</i>(6), e0178074. https://doi.org/10.1371/journal.pone.0178074"},{"id":"https://doi.org/10.1038/s41586-022-05543-x","type":"JournalArticle","unstructured":"Park,
        M., Leahey, E., &amp; Funk, R. J. (2023). Papers and patents are becoming
        less disruptive over time. <i>Nature</i>, <i>613</i>(7942), 138\u2013144.
        https://doi.org/10.1038/s41586-022-05543-x"},{"id":"https://doi.org/10.17705/1jais.00873","type":"JournalArticle","unstructured":"Avital,
        M., &amp; Copenhagen Business School. (2024). Digital Transformation of Academic
        Publishing: A Call for the Decentralization and Democratization of Academic
        Journals. <i>Journal of the Association for Information Systems</i>, <i>25</i>(1),
        172\u2013181. https://doi.org/10.17705/1jais.00873"},{"id":"https://doi.org/10.7554/elife.47338","type":"JournalArticle","unstructured":"McKiernan,
        E. C., Schimanski, L. A., Mu\u00f1oz Nieves, C., Matthias, L., Niles, M. T.,
        &amp; Alperin, J. P. (2019). Use of the Journal Impact Factor in academic
        review, promotion, and tenure evaluations. <i>eLife</i>, <i>8</i>. https://doi.org/10.7554/elife.47338"},{"id":"https://doi.org/10.1093/gigascience/giz053","type":"JournalArticle","unstructured":"Fire,
        M., &amp; Guestrin, C. (2019). Over-optimization of academic publishing metrics:
        observing Goodhart''s Law in action. <i>GigaScience</i>, <i>8</i>(6). https://doi.org/10.1093/gigascience/giz053"},{"id":"https://doi.org/10.3389/bjbs.2024.12054","type":"JournalArticle","unstructured":"Drozdz,
        J. A., &amp; Ladomery, M. R. (2024). The Peer Review Process: Past, Present,
        and Future. <i>British Journal of Biomedical Science</i>, <i>81</i>. https://doi.org/10.3389/bjbs.2024.12054"},{"id":"https://doi.org/10.3389/fnhum.2018.00037","type":"JournalArticle","unstructured":"Brembs,
        B. (2018). Prestigious Science Journals Struggle to Reach Even Average Reliability.
        <i>Frontiers in Human Neuroscience</i>, <i>12</i>. https://doi.org/10.3389/fnhum.2018.00037"},{"id":"https://doi.org/10.3389/fnhum.2013.00291","type":"JournalArticle","unstructured":"Brembs,
        B., Button, K., &amp; Munaf\u00f2, M. (2013). Deep impact: unintended consequences
        of journal rank. <i>Frontiers in Human Neuroscience</i>, <i>7</i>. https://doi.org/10.3389/fnhum.2013.00291"},{"id":"https://doi.org/10.1177/0192623320920209","type":"JournalArticle","unstructured":"Elmore,
        S. A., &amp; Weston, E. H. (2020). Predatory Journals: What They Are and How
        to Avoid Them. <i>Toxicologic Pathology</i>, <i>48</i>(4), 607\u2013610. https://doi.org/10.1177/0192623320920209"},{"id":"https://doi.org/10.1002/leap.1406","type":"JournalArticle","unstructured":"Repiso,
        R., Segarra\u2010Saavedra, J., Hidalgo\u2010Mar\u00ed, T., &amp; Tur\u2010Vi\u00f1es,
        V. (2021). The prevalence and impact of special issues in communications journals
        2015\u20132019. <i>Learned Publishing</i>, <i>34</i>(4), 593\u2013601. https://doi.org/10.1002/leap.1406"},{"id":"https://doi.org/10.12688/f1000research.16493.1","type":"JournalArticle","unstructured":"Schimanski,
        L. A., &amp; Alperin, J. P. (2018). The evaluation of scholarship in academic
        promotion and tenure processes: Past, present, and future. <i>F1000Research</i>,
        <i>7</i>, 1605. https://doi.org/10.12688/f1000research.16493.1"},{"id":"https://doi.org/10.1001/jama.2023.12500","type":"JournalArticle","unstructured":"Flanagin,
        A., Kendall-Taylor, J., &amp; Bibbins-Domingo, K. (2023). Guidance for Authors,
        Peer Reviewers, and Editors on Use of AI, Language Models, and Chatbots. <i>JAMA</i>,
        <i>330</i>(8), 702. https://doi.org/10.1001/jama.2023.12500"},{"id":"https://doi.org/10.1111/hequ.12511","type":"JournalArticle","unstructured":"Horta,
        H., &amp; Jung, J. (2024). The crisis of peer review: Part of the evolution
        of science. <i>Higher Education Quarterly</i>, <i>78</i>(4). https://doi.org/10.1111/hequ.12511"},{"id":"https://doi.org/10.1186/s12874-019-0688-x","type":"JournalArticle","unstructured":"Superchi,
        C., Gonz\u00e1lez, J. A., Sol\u00e0, I., Cobo, E., Hren, D., &amp; Boutron,
        I. (2019). Tools used to assess the quality of peer review reports: a methodological
        systematic review. <i>BMC Medical Research Methodology</i>, <i>19</i>(1).
        https://doi.org/10.1186/s12874-019-0688-x"},{"id":"https://doi.org/10.18665/sr.320210","type":"Report","unstructured":"Bergstrom,
        T., Rieger, O. Y., &amp; Schonfeld, R. C. (2024). <i>The Second Digital Transformation
        of Scholarly Publishing: Strategic Context and Shared Infrastructure</i>.
        Ithaka S+R. https://doi.org/10.18665/sr.320210"},{"id":"https://doi.org/10.7551/mitpress/12200.003.0017","type":"BookChapter","unstructured":"Alperin,
        J. P., Schimanski, L. A., La, M., Niles, M. T., &amp; McKiernan, E. C. (2022).
        The Value of Data and Other Non-traditional Scholarly Outputs in Academic
        Review, Promotion, and Tenure in Canada and the United States. In <i>The Open
        Handbook of Linguistic Data Management</i> (pp. 171\u2013182). The MIT Press.
        https://doi.org/10.7551/mitpress/12200.003.0017"},{"id":"https://doi.org/10.1371/journal.pbio.3002502","type":"JournalArticle","unstructured":"Avissar-Whiting,
        M., Belliard, F., Bertozzi, S. M., Brand, A., Brown, K., Cl\u00e9ment-Stoneham,
        G., Dawson, S., Dey, G., Ecer, D., Edmunds, S. C., Farley, A., Fischer, T.
        D., Franko, M., Fraser, J. S., Funk, K., Ganier, C., Harrison, M., Hatch,
        A., Hazlett, H., \u2026 Williams, M. (2024). Recommendations for accelerating
        open preprint peer review to improve the culture of science. <i>PLOS Biology</i>,
        <i>22</i>(2), e3002502. https://doi.org/10.1371/journal.pbio.3002502"},{"id":"https://doi.org/10.22323/2.20030401","type":"JournalArticle","unstructured":"Bucchi,
        M., &amp; Trench, B. (2021). Rethinking science communication as the social
        conversation around science. <i>Journal of Science Communication</i>, <i>20</i>(03),
        Y01. https://doi.org/10.22323/2.20030401"},{"id":"https://doi.org/10.1371/journal.pone.0265506","type":"JournalArticle","unstructured":"Dawson,
        D. (DeDe) ., Morales, E., McKiernan, E. C., Schimanski, L. A., Niles, M. T.,
        Alperin, J. P., &amp; Kirgiz, M. S. (2022). The role of collegiality in academic
        review, promotion, and tenure. <i>PLOS ONE</i>, <i>17</i>(4), e0265506. https://doi.org/10.1371/journal.pone.0265506"}],"registered_at":0,"relationships":[{"type":"HasReview","urls":["https://metaror.org/kotahi/articles/40"]}],"rid":"81deq-vqv03","subfield":null,"summary":"Academia
        is undergoing a rapid transformation characterized by exponential growth of
        scholarly outputs. This phenomenon, often termed the \"firehose problem,\"
        presents significant challenges for researchers, publishers, funders, policymakers,
        and institutions alike.","tags":["Thought Pieces"],"title":"Drinking from
        the Firehose?\nWrite More and Publish Less","topic":null,"topic_score":0,"updated_at":1742282842,"url":"https://upstream.force11.org/drinking-from-the-firehose/","version":"v1"}

        '
    headers:
      content-encoding:
      - gzip
      content-type:
      - application/json
      date:
      - Mon, 01 Dec 2025 17:18:52 GMT
      fly-request-id:
      - 01KBDES8ZBXKHXXZXFH3T9B72Y-ams
      ratelimit-limit:
      - '15'
      ratelimit-remaining:
      - '0'
      ratelimit-reset:
      - '57'
      server:
      - Fly/0dca8e971 (2025-11-29)
      transfer-encoding:
      - chunked
      vary:
      - Origin
      via:
      - 1.1 fly.io, 1.1 fly.io
    status:
      code: 200
      message: ''
version: 1
