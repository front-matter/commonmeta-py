interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - python-requests/2.32.5
    method: GET
    uri: https://api.rogue-scholar.org/posts/10.54900/zg929-e9595
  response:
    body:
      string: '{"abstract":"The New York Times ushered in the New Year with a lawsuit
        against OpenAI and Microsoft. The paper covered the suit, fittingly, as a
        major business story.","archive_url":"https://wayback.archive-it.org/22124/2025-05-05T10:39:18Z/https://upstream.force11.org/large-language-publishing/","authors":[{"contributor_roles":[],"family":"Pooley","given":"Jeff","url":"https://orcid.org/0000-0002-3674-1930"}],"blog":{"archive_collection":22124,"archive_host":null,"archive_prefix":"https://wayback.archive-it.org/22124/20231105103706/","archive_timestamps":[20231105103706,20240505132151,20241105103657,20250505103918],"authors":null,"canonical_url":null,"category":"humanities","community_id":"aeaafcbb-94b5-477a-a89f-8fba5925e926","created_at":1673568000,"current_feed_url":"https://upstream.force11.org/atom/","description":"The
        community blog for all things Open Research.","doi_as_guid":false,"favicon":"https://rogue-scholar.org/api/communities/b56ef314-34f7-4c7f-b0e2-d0bf13bfe83b/logo","feed_format":"application/atom+xml","feed_url":"https://upstream.force11.org/atom-complete/","filter":null,"funding":null,"generator":"Ghost","generator_raw":"Ghost
        5.25","home_page_url":"https://upstream.force11.org","id":"e3952730-ffb7-4ef9-b4a5-6433d86b2819","indexed":false,"issn":null,"language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","mastodon":"https://scicomm.xyz/@upstream","prefix":"10.54900","registered_at":1729244339,"relative_url":null,"ror":null,"secure":true,"slug":"upstream","status":"active","subfield":"1802","subfield_validated":null,"title":"Upstream","updated_at":1764663321,"use_api":true,"use_mastodon":false,"user_id":"08014cf6-3335-4588-96f4-c77ac1e535b2"},"blog_name":"Upstream","blog_slug":"upstream","citations":[{"citation":"https://doi.org/10.1016/j.compcom.2024.102892","published_at":"2025-03","unstructured":"Pandey,
        H. L., Bhusal, P. C., &amp; Niraula, S. (2025). Large language models and
        digital multimodal composition in the first-year composition classrooms: An
        encroachment and/or enhancement dilemma. <i>Computers and Composition</i>,
        <i>75</i>, 102892. https://doi.org/10.1016/j.compcom.2024.102892","updated_at":"2025-02-02T19:19:34.824105+00:00","validated":true},{"citation":"https://doi.org/10.1515/bfp-2024-0008","published_at":"2024-05-16","unstructured":"Siems,
        R. (2024). Subprime Impact Crisis. Bibliotheken, Politik und digitale Souver\u00e4nit\u00e4t.
        <i>Bibliothek Forschung Und Praxis</i>, <i>48</i>(2), 311\u2013321. https://doi.org/10.1515/bfp-2024-0008","updated_at":"2025-02-02T19:19:54.360906+00:00","validated":true},{"citation":"https://doi.org/10.1515/bfp-2025-0002","published_at":"2025-06-28","unstructured":"Lahrsow,
        M. (2025). KI-Tools f\u00fcr die wissenschaftliche Literaturrecherche: Potenziale,
        Problematiken, Didaktik und Zukunftsperspektiven. <i>Bibliothek Forschung
        Und Praxis</i>, <i>49</i>(2), 230\u2013253. https://doi.org/10.1515/bfp-2025-0002","updated_at":"2025-08-02T16:42:09.197026+00:00","validated":true},{"citation":"https://doi.org/10.53731/2ych7-jqc35","published_at":"2024-10-07","unstructured":"Fenner,
        M. (2024, October 7). Rogue Scholar learns about communities. <i>Front Matter</i>.
        https://doi.org/10.53731/2ych7-jqc35","updated_at":"2025-10-22T08:23:27.906816+00:00","validated":true},{"citation":"https://doi.org/10.53731/dv8z6-a6s33","published_at":"2024-10-07","unstructured":"Fenner,
        M. (2024, October 7). Rogue Scholar learns about communities. <i>Front Matter</i>.
        https://doi.org/10.53731/dv8z6-a6s33","updated_at":"2025-02-02T19:18:46.946136+00:00","validated":true},{"citation":"https://doi.org/10.54900/sdgvx-tty28","published_at":"2024-01-16","unstructured":"Bhosale,
        U., Phadke, G., &amp; Kapadia, A. (2024, January 16). Need for Consistent
        and Uniform Guidelines Regarding the Use and Disclosure of AI-generated Scholarly
        Content. <i>Upstream</i>. https://doi.org/10.54900/sdgvx-tty28","updated_at":"2025-02-02T19:18:45.845526+00:00","validated":true},{"citation":"https://doi.org/10.54900/z39at-we393","published_at":"2024-01-16","unstructured":"Bhosale,
        U., Phadke, G., &amp; Kapadia, A. (2024, January 16). Need for Consistent
        and Uniform Guidelines Regarding the Use and Disclosure of AI-generated Scholarly
        Content. <i>Upstream</i>. https://doi.org/10.54900/z39at-we393","updated_at":"2025-10-22T08:33:36.767181+00:00","validated":true}],"content_html":"<p><em>The
        New York Times</em> ushered in the New Year with a <a href=\"https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf\">lawsuit</a>
        against OpenAI and Microsoft. The paper <a href=\"https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html\">covered
        the suit</a>, fittingly, as a major business story. OpenAI and its Microsoft
        patron had, according to the filing, stolen \u201c<em>millions</em> of The
        Times\u2019 copyrighted news articles, in-depth investigations, opinion pieces,
        reviews, how-to guides,\u201d and more\u2014all to train OpenAI\u2019s large
        language models (LLMs). The <em>Times</em> sued to stop the tech companies\u2019
        \u201cfree-ride\u201d on the newspaper\u2019s \u201cuniquely valuable\u201d
        journalism.</p>\n<p>OpenAI and Microsoft have, of course, cited fair use to
        justify their permissionless borrowing. Across 70 bitter pages, the <em>Times</em>\u2019
        lawyers drove a bulldozer through all <a href=\"https://en.wikipedia.org/wiki/Fair_use\">four
        factors</a> that U.S. judges weigh for fair use. The brief also points to
        reputational harm\u2014from made-up responses that ChatGPT or Bing Chat attribute
        to the <em>Times</em>: \u201cIn plain English, it\u2019s misinformation.\u201d</p>\n<p>There\u2019s
        no question that attorneys for Elsevier and the other scholarly-publishing
        giants are reading the <em>Times</em> <a href=\"https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf\">filing</a>
        carefully. They\u2019ll notice a leitmotif: The newspaper\u2019s expensively
        reported stories produce <em>trusted</em> knowledge, in otherwise short supply.
        In a \u201cdamaged information ecosystem [\u2026] awash in unreliable content,\u201d
        the <em>Times</em>\u2019 journalism is an \u201cexceptionally valuable body
        of data\u201d for AI training, states the filing. Other news organizations
        have the same view; some have <a href=\"https://www.wsj.com/business/media/openai-to-pay-politico-parent-axel-springer-for-using-its-content-bdc33332\">signed
        licensing deals</a>, while others are <a href=\"https://www.nytimes.com/2023/12/29/business/media/media-openai-chatgpt.html\">negotiating</a>
        with OpenAI and its peers. No more free rides.</p>\n<p>The big scholarly publishers
        are very likely to agree. And they\u2019re sitting on the <em>other</em> corpus
        of vetted knowledge, science and scholarship. So licensing talks are almost
        certainly underway, with threats and lawsuits doubtlessly prepped. At the
        same time, the commercial publishers are building their own AI products. In
        the year since ChatGPT\u2019s splashy entrance, at least three of the big
        five scholarly publishers, plus Clarivate, have announced tools and features
        powered by LLMs. They are joined by dozens of VC-backed startups\u2014acquisition
        targets, one and all\u2014promising an AI boost across the scholarly workflow,
        from literature search to abstract writing to manuscript editing.</p>\n<p>Thus
        the two main sources of trustworthy knowledge, science and journalism, are
        poised to extract protection money\u2014to otherwise exploit their vast pools
        of vetted text as \u201ctraining data.\u201d But there\u2019s a key difference
        between the news and science: Journalists\u2019 salaries, and the cost of
        reporting, are covered by the companies. Not so for scholarly publishing:
        Academics, of course, write and review for free, and much of our research
        is funded by taxpayers. The <em>Times</em> suit is marinated in complaints
        about the costly business of journalism. The likes of Taylor &amp; Francis
        and Springer Nature won\u2019t have that argument to make. It\u2019s hard
        to call out free-riding when it''s your own business model.</p>\n<h2 id=\"surveillance-publishing-llm-edition\">Surveillance
        Publishing, LLM Edition</h2>\n<p>The AI hype-cycle froth has come for scholarly
        publishing. The industry\u2019s feverish\u2014if mostly aspirational\u2014embrace
        of AI should be read as the latest installment of an ongoing campaign.<sup
        class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup> Led by
        Elsevier, commercial publishers have, for about a decade, layered another
        business on top of their legacy publishing operations. That business is to
        mine and process scholars\u2019 works and behavior into prediction products,
        sold back to universities and research agencies. Elsevier, for example, peddles
        a dashboard software, Pure, to university assessment offices\u2014one that
        assigns each of the school\u2019s researchers a Fingerprint\u00ae of weighted
        keywords. The underlying data comes from Elsevier\u2019s Scopus, the firm\u2019s
        propriety database of abstracts and citations. Thus the scholar <em>is</em>
        the product: Her articles and references feed Scopus and Pure, which are then
        sold back to her university employer. That same university, of course, already
        shells out usurious subscription and APC dollars to Elsevier\u2014which, in
        a painful irony, have financed the very acquisition binge that transformed
        the firm into a <a href=\"https://www.hachettebookgroup.com/titles/shoshana-zuboff/the-age-of-surveillance-capitalism/9781610395694/?lens=publicaffairs\">full-stack
        publisher</a>.</p>\n<p>Elsevier and the other big publishers are, to borrow
        Sarah Lamdan\u2019s phrase, <a href=\"https://www.sup.org/books/title/?id=33205\">data
        cartels</a>. I\u2019ve called this drive to extract profit from researchers\u2019
        behavior <a href=\"https://elephantinthelab.org/surveillance-publishing/\"><em>surveillance
        publishing</em></a>\u2014by analogy to Shoshana Zuboff\u2019s notion of <a
        href=\"https://www.hachettebookgroup.com/titles/shoshana-zuboff/the-age-of-surveillance-capitalism/9781610395694/?lens=publicaffairs\">surveillance
        capitalism</a>, in which firms like Google and Meta package user data to sell
        to advertisers. The core business strategy is the same for Silicon Valley
        and Elsevier: extract data from behavior to feed predictive models that, in
        turn, get refined and sold to customers. In one case it\u2019s Facebook posts
        and in the other abstracts and citations, but either way the point is to mint
        money from the by-products of (consumer or scholarly) behavior. One big difference
        between the big tech firms and the publishers is that Google et al entice
        users with free services like Gmail: If you\u2019re not paying for it, the
        adage goes, then you\u2019re the product. In the Elsevier case we\u2019re
        the product <em>and</em> we\u2019re paying (a lot) for it.</p>\n<p>Elsevier
        and some of the other big publishers <a href=\"https://www.elsevier.com/about/press-releases/elsevier-introduces-authoritative-scientific-datasets-to-fuel-innovation-and\">already
        harness</a> their troves of scholarly data to, for example, assign subject
        keywords to scholars and works. They have, indeed, been using so-called AI
        for years now, including variations on the machine-learning (ML) techniques
        ascendant in the last 15 years or so. What\u2019s different about the publishers\u2019
        imminent licensing windfall and wave of announced tools is, in a word, ChatGPT.
        It\u2019s true that successive versions of enormous, \u201clarge language\u201d
        models from OpenAI, Google, and others have been kicking around in commercial
        and academic circles for years. But the November 2022 public release of ChatGPT
        changed the game. Among other things, and almost overnight, the value of <em>content</em>
        took on a different coloration. Each of the giant \u201cfoundation\u201d models,
        including OpenAI\u2019s GPT series, is fed on prodigious helpings of text.
        The appetite for such training data isn\u2019t sated, even as the legality
        of the ongoing ingestion is an open and litigated question.</p>\n<p>The big
        publishers think they\u2019re sitting on a gold mine. It\u2019s not just their
        paywalled, full-text scholarship, but also the reams of other data they hoover
        up from academics across their platforms and products. In theory at least,
        their proprietary content is\u2014unlike the clown show of the open web\u2014vetted
        and linked. On those grounds, observers have declared that publishers may
        be the <a href=\"https://www.forbes.com/sites/alexzhavoronkov/2023/02/23/the-unexpected-winners-of-the-chatgpt-generative-ai-revolution/?sh=45a4d82312b0&amp;ref=lorcandempsey.net\">\u201cbiggest
        winners\u201d in the generative AI revolution</a>. Maybe. But either way,
        expect Springer Nature, Taylor &amp; Francis, Elsevier, Wiley, and SAGE to
        test the theory.</p>\n<h2 id=\"hallucinating-parrots\">Hallucinating Parrots</h2>\n<p>Truly
        large language models, like those driving ChatGPT and Bard, are notorious
        fabulists. They routinely, and confidently, return what the industry euphemism
        terms \u201challucinations.\u201d Some observers expect the problem to keep
        getting worse, as LLM-generated material floods the internet. The big models,
        on this fear, will feed on their own falsehood-ridden prose in subsequent
        training rounds\u2014a kind of large-language cannibalism that, over time,
        could crowd out whatever share of the pre-LLM web that was more-or-less truthful.</p>\n<p>One
        solution to the problem, with gathering VC and hype-cycle momentum, is a turn
        to so-called \u201csmall\u201d language models. The idea is to apply the same
        pattern-recognition techniques, but on curated, domain-specific data sets.
        One advantage of the smaller models, according to proponents, is their ability
        to restrict training data to the known and verifiable. The premise is that,
        with less garbage in, there will be less garbage out.</p>\n<p>So it\u2019s
        no surprise that the <a href=\"https://www.timeshighereducation.com/blog/academic-chatgpt-needs-better-schooling\">published
        scientific record</a> has emerged, in the industry chatter, as an especially
        promising hallucination-slayer. Here\u2019s a body of vetted knowledge, the
        thinking goes, cordoned off from the internet\u2019s Babelist free-for-all.
        What makes the research corpus different is, well, peer review and editorial
        gate-keeping, together with citation conventions and scholars\u2019 putative
        commitment to a culture of self-correcting criticism. Thus the published record
        is\u2014among bodies of minable text\u2014uniquely trustworthy. Or that\u2019s
        what small-language evangelists are claiming.</p>\n<p>Enter <a href=\"https://diginomica.com/reed-elsevier-sees-promise-small-language-models-and-graph-data\">Elsevier</a>
        and its oligopolistic peers. They guard (with paywalled vigilance) a large
        share of published scholarship, much of which is unscrapable. A growing proportion
        of their total output is, it\u2019s true, open access, but a large share of
        that material carries a non-commercial license. Standard OA agreements tend
        to grant publishers blanket rights, so they <a href=\"https://scholarlykitchen.sspnet.org/2023/03/07/some-thoughts-on-five-pending-ai-litigations-avoiding-squirrels-and-other-ai-distractions/\">have
        a claim</a>\u2014albeit one contested on fair-use grounds by OpenAI and the
        like\u2014to exclusive exploitation. Even the balance of OA works that permit
        commercial re-use are corralled with the rest, on propriety platforms like
        Elsevier\u2019s ScienceDirect. Those platforms <a href=\"https://sparcopen.org/news/2023/sparc-report-urges-action-to-address-concerns-with-sciencedirect-data-privacy-practices/\">also
        track researcher behavior</a>, like downloads and citations, that can be used
        to tune their models\u2019 outputs. Such models could, in theory, be fed by
        proprietary bibliographic platforms, such as Clarivate\u2019s Web of Science,
        Elsevier\u2019s Scopus, and Digital Science\u2019s Dimensions (owned by Springer
        Nature\u2019s parent company).</p>\n<h2 id=\"%E2%80%98the-world%E2%80%99s-largest-collection%E2%80%99\">\u2018The
        World\u2019s Largest Collection\u2019</h2>\n<p>One area where a number of
        big publishers are already jumping in is <a href=\"https://www.nature.com/articles/d41586-023-02470-3\">search-based
        summary</a>. Elsevier is piloting <a href=\"https://beta.elsevier.com/products/scopus/scopus-ai\">Scopus
        AI</a>, with an early 2024 launch expected. Researchers type in natural-language
        questions, and they get a summary spit out, with some suggested follow up
        questions and references\u2014those open a ScienceDirect view in the sidebar.
        ScopusAI results also include a \u201cConcept Map\u201d\u2014an expandable
        topic-based tree, presumably powered by the firm\u2019s Fingerprint keywords.</p>\n<p>The
        tool is combing its Scopus titles and abstracts\u2014from 2018 on\u2014and
        then feeding the top 10 or so results into an OpenAI GPT model for summarizing.
        Elsevier isn\u2019t shy about its data-trove advantage: Scopus AI is \u201cbuilt
        on the world\u2019s largest collection of trusted peer-reviewed academic literature,\u201d
        proclaims a splashy <a href=\"https://www.elsevier.com/products/scopus/scopus-ai\">promotional
        video</a>.</p>\n<p>Springer Nature and Clarivate are also in on the search-summary
        game. Dimensions, the Scopus competitor from Springer Nature\u2019s corporate
        sibling, has a <a href=\"https://www.dimensions.ai/discover-dimensions-ai-assistant/\">Dimensions
        AI Assistant</a> in trials. Like Scopus AI, the Dimensions tool is retrieving
        a small number of abstracts based on conversational search, turning to models
        from OpenAI and Google for the summaries.</p>\n<p>Meanwhile, Clarivate\u2014which
        owns Web of Science and ProQuest\u2014has <a href=\"https://ir.clarivate.com/news-events/press-releases/news-details/2023/Clarivate-Announces-Partnership-with-AI21-Labs-as-part-of-its-Generative-AI-Strategy-to-Drive-Growth/default.aspx\">struck
        a deal</a> with <a href=\"https://www.ai21.com\">AI21 Labs</a>, an Israeli
        LLM startup (tagline: \u201cWhen Machines Become Thought Partners\u201d).
        Using Clarivate\u2019s \u201ctrusted content as the foundation,\u201d AI21
        promises to use its models to generate \u201chigh-quality, contextual-based
        answers and services,\u201d with what it frankly calls \u201cClarivate\u2019s
        troves of content and data.\u201d</p>\n<p>The big firms will be competing
        with a stable of VC-backed startups, including <a href=\"https://ought.org/elicit\">Ought</a>
        (\u201cScale up good reasoning\u201d), <a href=\"https://iris.ai\">iris.ai</a>
        (\u201cThe Researcher Workspace\u201d), <a href=\"https://scisummary.com\">SciSummary</a>
        (\u201cUse AI to summarize scientific articles in seconds\u201d), <a href=\"https://www.petal.org\">Petal</a>
        (\u201cChat with your documents\u201d), <a href=\"https://jenni.ai\">Jenni</a>
        (\u201cSupercharge Your Next Research Paper\u201d), <a href=\"https://www.scholarcy.com\">scholarcy</a>
        (\u201cThe AI-powered article summarizer\u201d), <a href=\"https://imagetwin.ai\">Imagetwin</a>
        (\u201cIncrease the Quality in Science\u201d), <a href=\"https://keenious.com\">keenious</a>
        (\u201cFind research relevant to any document!\u201d), and <a href=\"https://consensus.app/home/about-us/\">Consensus</a>
        (\u201cAI Search Engine for Research\u201d).</p>\n<p>An open question is if
        the startups can compete with the big publishers; many are using the open
        access database <a href=\"https://www.semanticscholar.org\">Semantic Scholar</a>,
        which <a href=\"https://allenai.org/data/s2orc\">excludes the full-text</a>
        of paywalled articles. They\u2019ve won plenty of venture-capital backing,
        but\u2014if the wider AI industry is any guide\u2014the startups will face
        an uphill climb to stay independent. Commercial AI, after all, is dominated
        by a <a href=\"https://www.technologyreview.com/2023/12/05/1084393/make-no-mistake-ai-is-owned-by-big-tech/\">handful
        of giant U.S. and Chinese corporations</a>, nearly all <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807\">big
        tech incumbents</a>. The industry has ferocious economies of scale, largely
        because model-building takes enormous financial and human resources.</p>\n<p>The
        big publishers may very well find themselves in a similar pole position. The
        firms\u2019 stores of proprietary full-text papers and other privately held
        data are a built-in advantage. Their astronomical margins on legacy subscription-and-APC
        publishing businesses means that they have the capital at hand to invest and
        acquire. Elsevier\u2019s decade-long acquisition binge was, in that same way,
        financed by its lucrative earnings. There\u2019s every reason to expect that
        the company will fund its costly LLM investments from the same surplus; Elsevier\u2019s
        peers are likely to follow suit. Thus universities and taxpayers are serving,
        in effect, as a capital fund for AI products that, in turn, will be sold back
        to us. The independent startups may well be acquired along the way. The giant
        publishers <em>themselves</em> may be acquisition targets to the even-larger
        Silicon Valley firms hungry for training data\u2014as Avi Staiman <a href=\"https://scholarlykitchen.sspnet.org/2023/08/08/will-building-llms-become-the-new-revenue-driver-for-academic-publishing/\">recently
        observed</a> in <em>The Scholarly Kitchen</em>.</p>\n<p>The acquisition binge
        has already begun. In October Springer Nature <a href=\"https://group.springernature.com/gp/group/media/press-releases/acquisition-slimmer-ai-science-division/26215608\">acquired</a>
        the Science division of <a href=\"https://www.slimmer.ai\">Slimmer AI</a>,
        a Dutch \u201cAI venture studio\u201d that the publisher has worked with since
        2015 on peer review and plagiarism-detection tools. Digital Science, meanwhile,
        <a href=\"https://www.researchinformation.info/news/digital-science-acquires-ai-service-writefull?utm_campaign=RI%20Newsline%2028-11-23&amp;utm_content=https%3A%2F%2Fwww.researchinformation.info%2Fnews%2Fdigital-science-acquires-ai-service-writefull&amp;utm_term=Research%20Information&amp;utm_medium=email&amp;utm_source=Adestra\">just
        bought</a> <a href=\"https://www.writefull.com\">Writefull</a>, which makes
        an academic writing assistant (to join corporate sibling Springer Nature\u2019s
        recently announced <a href=\"https://www.aje.com/curie/\">Curie</a>). Digital
        Science pitched the acquisition as a small-language model play: \u201cWhile
        the broader focus is currently on LLMs,\u201d said a company executive in
        the <a href=\"https://www.researchinformation.info/news/digital-science-acquires-ai-service-writefull?utm_campaign=RI%20Newsline%2028-11-23&amp;utm_content=https%3A%2F%2Fwww.researchinformation.info%2Fnews%2Fdigital-science-acquires-ai-service-writefull&amp;utm_term=Research%20Information&amp;utm_medium=email&amp;utm_source=Adestra\">press
        release</a>, \u201cWritefull\u2019s small, specialized models offer more flexibility,
        at lower cost, with auditable metrics.\u201d Research Solutions, a Nevada
        company that sells access to the big commercial publishers\u2019 paywalled
        content to corporations, <a href=\"https://ai-techpark.com/research-solutions-announces-acquisition-of-scite/\">recently
        bought</a> <a href=\"https://scite.ai/assistant?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=brand&amp;utm_term=scite&amp;gclid=CjwKCAiAvJarBhA1EiwAGgZl0KEglv3ZDlnC2EAnHNERfv-5URJEcVa54TgCDEUXzn61SYrSVIVrhBoC_h4QAvD_BwE\">scite</a>,
        a startup whose novel offering\u2014<a href=\"https://direct.mit.edu/qss/article/2/3/882/102990/scite-A-smart-citation-index-that-displays-the\">citations
        <em>contexts</em></a>\u2014has been repackaged as \u201cChatGPT for Science.\u201d</p>\n<h2
        id=\"fair-use\">Fair Use?</h2>\n<p>As the <em>Times</em> lawsuit suggests,
        there\u2019s a big legal question mark hovering over the big publishers\u2019
        AI prospects. The key issue, winding its way through the courts, is fair use:
        Can the likes of OpenAI scrape up copyrighted content into their models, without
        permission or compensation? The Silicon Valley tech companies think so; they\u2019re
        <a href=\"https://jacobin.com/2023/11/artificial-intelligence-big-tech-lobbying-copyright-infringement-regulation/\">fresh
        converts</a> to fair-use maximalism, as revealed by their public comments
        filed with the US Copyright Office. The companies\u2019 \u201coverall message,\u201c
        reported <em>The Verge</em> in a <a href=\"https://www.theverge.com/2023/11/4/23946353/generative-ai-copyright-training-data-openai-microsoft-google-meta-stabilityai\">round-up</a>,
        is that they \u201cdon\u2019t think they should have to pay to train AI models
        on copyrighted work.\u201d Artists and other content creators have begged
        to differ, filing a handful of high-profile lawsuits.</p>\n<p>The publishers
        haven\u2019t filed their own suits yet, but they\u2019re certainly watching
        the cases carefully. Wiley, for one, <a href=\"https://www.nature.com/articles/d41586-023-03144-w\">told
        <em>Nature</em></a> that it was \u201cclosely monitoring industry reports
        and litigation claiming that generative AI models are harvesting protected
        material for training purposes while disregarding any existing restrictions
        on that information.\u201d The firm has called for audits and regulatory oversight
        of AI models, to address the \u201cpotential for unauthorised use of restricted
        content as an input for model training.\u201c Elsevier, for its part, has
        <a href=\"https://www.timeshighereducation.com/news/publishers-seek-protection-ai-mining-academic-research\">banned</a>
        the use of \u201cour content and data\u201d for training; its sister company
        LexisNexis, likewise, <a href=\"https://www.optica-opn.org/home/articles/volume_34/october_2023/features/generative_ai_meets_scientific_publishing/\">recently
        emailed customers</a> to \u201cremind\u201d them that feeding content to \u201clarge
        language models and generative AI\u201d is forbidden. CCC (n\u00e9e Copyright
        Clearance Center), in its <a href=\"https://scholarlykitchen.sspnet.org/2023/11/28/the-united-states-copyright-office-notice-of-inquiring-on-ai-a-quick-take/\">own
        comments to the US Copyright Office</a>, took a predictably muscular stance
        on the question:</p>\n<blockquote>\n<p>There is certainly enough copyrightable
        material available under license to build reliable, workable, and trustworthy
        AI. Just because a developer wants to use \u201ceverything\u201d does not
        mean it needs to do so, is entitled to do so, or has the right to do so. Nor
        should governments and courts twist or modify the law to accommodate them.</p>\n</blockquote>\n<p>The
        for-profit CCC is the publishing industry\u2019s main licensing and permission
        enforcer. Big tech and the commercial publishing giants are <a href=\"https://jacobin.com/2023/11/artificial-intelligence-big-tech-lobbying-copyright-infringement-regulation/\">already
        maneuvering for position</a>. As Joseph Esposito, a keen observer of scholarly
        publishing, <a href=\"https://scholarlykitchen.sspnet.org/2023/07/12/who-is-going-to-make-money-from-artificial-intelligence-in-scholarly-communications/\">put
        the point</a>: \u201cscientific publishers in particular, may have a special,
        remunerative role to play here.\u201d</p>\n<p>One near-term consequence may
        be a shift in the big publishers\u2019 approach to open access. The companies
        are already updating their licenses and terms to forbid commercial AI training\u2014for
        anyone but them, of course. The companies could also pull back from OA altogether,
        to keep a larger share of exclusive content to mine. Esposito made the argument
        explicit in a recent <em>Scholarly Kitchen</em> <a href=\"https://scholarlykitchen.sspnet.org/2023/07/12/who-is-going-to-make-money-from-artificial-intelligence-in-scholarly-communications/\">post</a>:
        \u201cThe unfortunate fact of the matter is that the OA movement and the people
        and organizations that support it have been co-opted by the tech world as
        it builds content-trained AI.\u201c Publishers need \u201cmore copyright protection,
        not less,\u201c he added. Esposito\u2019s consulting firm, in its latest <a
        href=\"https://www.ce-strategy.com/the-brief/gemini/\">newsletter</a>, called
        the liberal <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative
        Commons BY</a> license a \u201cmechanism to transfer value from scientific
        and scholarly publishers to the world\u2019s wealthiest tech companies.\u201c
        Perhaps, though I would preface the point: <em>Commercial scholarly publishing</em>
        is a mechanism to transfer value from scholars, taxpayers, universities to
        the world\u2019s most profitable companies.</p>\n<h2 id=\"the-matthew-effect-in-ai\">The
        Matthew Effect in AI</h2>\n<p>There are a hundred and one reasons to worry
        about Elsevier mining our scholarship to maximize its profits. I want to linger
        on what is, arguably, the most important: the potential effects on knowledge
        itself. At the core of these tools\u2014including a predictable avalanche
        of as-yet-unannounced products\u2014is a series of verbs: to surface, to rank,
        to summarize, and to recommend. The object of each verb is <em>us</em>\u2014our
        scholarship and our behavior. What\u2019s at stake is the kind of knowledge
        that the models surface, and <em>whose</em> knowledge.</p>\n<p>AI models are
        poised to serve as <a href=\"https://zenodo.org/records/7222845\">knowledge
        arbitrators</a>, by picking winners and losers according to what they make
        visible. There are two big and interlocking problems with this role: The models
        are trained on the past, and their filtering logic is inscrutable. As a result,
        they may smuggle in the many biases that mark the history of scholarship,
        around gender, geography, and other lines of difference. In this context it\u2019s
        useful to revive an old concept in the sociology of science. According to
        the Matthew Effect\u2014<a href=\"https://www.science.org/doi/10.1126/science.159.3810.56\">named
        by Robert Merton</a> decades ago\u2014prominent and well-cited scholars tend
        to receive still more prominence and citations. The flip side is that less-cited
        scholars tend to slip into further obscurity over time. (\u201cFor to every
        one who has will more be given, and he will have abundance; but from him who
        has not, even what he has will be taken away\u201d\u2014\u200aMatthew 25:29.)
        These dynamics of cumulative advantage have, in practice, served to amplify
        the knowledge system\u2019s patterned inequalities\u2014for example, in the
        case of gender and twentieth-century scholarship, aptly labeled the <a href=\"https://journals.sagepub.com/doi/abs/10.1177/030631293023002004\">Matilda
        Effect</a> by Margaret Rossiter.</p>\n<p>The deployment of AI models in science,
        especially proprietary ones, may produce a Matthew Effect on the scale of
        Scopus, and with no paper trail. The problem is analogous to the well-documented
        bias-smuggling with existing generative models; image tools trained on, for
        example, mostly white and male photos reproduce the skew in their prompt-generated
        outputs. With our bias-laden scholarship as training data, the academic models
        may spit out results that, in effect, double-down on inequality. What\u2019s
        worse is that we won\u2019t really know, due to the models\u2019 black-boxed
        character. Thus the tools may act as laundering machines\u2014context-erasing
        abstractions that disguise their probabilistic \u201creasoning.\u201d Existing
        biases, like male academics\u2019 propensity for self-citation, may win a
        fresh coat of algorithmic legitimacy. Or consider center-periphery dynamics
        along North-South and native-English-speaking lines: Gaps traceable to geopolitical
        history, including the legacy of European colonialism, may be buried still
        deeper. The models, in short, could serve as privilege multipliers.</p>\n<p>The
        AI models aren\u2019t going away, but we should demand that\u2014to whatever
        extent possible\u2014the tools and models are <a href=\"https://hdsr.mitpress.mit.edu/pub/xau9dza3/\">subject
        to scrutiny and study</a>. This means ruling out propriety products, unless
        they can be pried open by law or regulation. We should, meanwhile, bring the
        models in-house, within the academic fold, using mission-aligned collections
        like the Open University\u2019s <a href=\"https://core.ac.uk/about\">CORE</a>
        and the Allen Institute\u2019s <a href=\"https://www.semanticscholar.org\">Semantic
        Scholar</a>. Academy-led efforts to build <a href=\"https://blog.allenai.org/announcing-ai2-olmo-an-open-language-model-made-by-scientists-for-scientists-ab761e4e9b76\">nonprofit</a>
        <a href=\"https://blog.core.ac.uk/2023/03/17/core-gpt-combining-open-access-research-and-ai-for-credible-trustworthy-question-answering/\">models</a>
        and <a href=\"https://www.semanticscholar.org/product/semantic-reader\">tools</a>
        should be transparent, explainable, and auditable.</p>\n<h2 id=\"stop-tracking-scholarship\">Stop
        Tracking Scholarship</h2>\n<p>These are early days. The legal uncertainty,
        the vaporware, the breathless annual-report prose: All of it points to aspiration
        and C-suite prospecting. We\u2019re not yet living in a world of publisher
        small-language models, trained on our work and behavior.</p>\n<p>Still, I\u2019m
        convinced that the big five publishers, plus Clarivate, will make every effort
        to pad their margins with fresh AI revenue. My guess is that they\u2019ll
        develop and acquire their way to a product portfolio up, down, and around
        the research-lifecycle, on Elsevier\u2019s existing full-stack model. After
        all\u2014and depending on what we mean by AI\u2014the commercial publishers
        have been rolling out AI products for years. Every signal suggests they\u2019ll
        pick up the pace, with hype-driven pursuit of GPT-style language models in
        particular. They\u2019ll sell their own products back to us, and\u2014I predict\u2014license
        our papers out to the big foundation models, court-willing.</p>\n<p>So it\u2019s
        an urgent task to push back now, and not wait until after the models are trained
        and deployed. What\u2019s needed is a full-fledged campaign, leveraging activism
        and legislative pressure, to challenge the commercial publishers\u2019 extractive
        agenda. One crucial framing step is to treat the impending AI avalanche as
        continuous with\u2014as an extension of\u2014the publishers\u2019 in-progress
        mutation into surveillance-capitalist data businesses. The surveillance publisher
        era was symbolically kicked off in 2015, when Reed-Elsevier adopted its \u201cshorter,
        more modern name\u201d RELX Group to <a href=\"https://www.relx.com/~/media/Files/R/RELX-Group/documents/reports/annual-reports/2015-annual-report.pdf\">mark</a>
        its \u201ctransformation\u201d from publisher to \u201ctechnology, content
        and analytics driven business.\u201d They\u2019ve made good on the promise,
        skimming scholars\u2019 behavioral cream with product-by-product avidity.
        Clarivate and Elsevier\u2019s peers have followed their lead.</p>\n<p>Thus
        the turn to AI is more of the same, only more so. The publishers\u2019 cocktail
        of probability, prediction, and profit is predicated on the same process:
        extract our scholarship and behavior, then sell it back to us in congealed
        form. The stakes are higher given that some of the publishers are embedded
        in data-analytics conglomerates\u2014RELX (Elsevier) and Informa (Taylor &amp;
        Francis), joined by publisher-adjacent firms like Clarivate and Thomson Reuters.
        Are the companies cross-pollinating their academic and \u201crisk solutions\u201d
        businesses? RELX\u2019s LexisNexis sold face-tracking and other surveillance
        tools to the US Customs and Border Protection last year, as <em>The Intercept</em>
        <a href=\"https://theintercept.com/2023/11/16/lexisnexis-cbp-surveillance-border\">recently
        reported</a>. As SPARC (the library alliance) put it in its <a href=\"https://zenodo.org/doi/10.5281/zenodo.10078609\">November
        report</a> on Elsevier\u2019s ScienceDirect platform: \u201cThere is little
        to nothing to stop vendors who collect and track [library] patron data from
        feeding that data\u2014either in its raw form or in aggregate\u2014into their
        data brokering business.\u201d</p>\n<p>So far the publishers\u2019 data-hoovering
        hasn\u2019t galvanized scholars to protest. The main reason is that most academics
        are blithely unaware of the tracking\u2014no surprise, given scholars\u2019
        too-busy-to-care ignorance of the publishing system itself. The library community
        is far more attuned to the unconsented pillage, though librarians\u2014aside
        from <a href=\"https://sparcopen.org/news/2021/addressing-the-alarming-systems-of-surveillance-built-by-library-vendors/\">SPARC</a>\u2014haven\u2019t
        organized on the issue. There have been scattered notes of dissent, including
        a <a href=\"https://stoptrackingscience.eu\">Stop Tracking Science</a> petition,
        and an <a href=\"https://www.scienceguide.nl/2020/06/open-science-deal-benefits-elsevier/\">outcry</a>
        from Dutch scholars on a 2020 <a href=\"https://www.elsevier.com/about/press-releases/corporate/dutch-research-institutions-and-elsevier-initiate-worlds-first-national-open-science-partnership\">data-and-publish
        agreement</a> with Elsevier, largely because the company had baked its prediction
        products into the deal. In 2022 the German national research foundation, Deutsche
        Forschungsgemeinschaft (DFG), released its own <a href=\"https://www.dfg.de/resource/blob/174924/d99b797724796bc1a137fe3d6858f326/datentracking-papier-en-data.pdf\">report-cum-warning</a>\u2014\u201cindustrialization
        of knowledge through tracking,\u201d in the report\u2019s words. Sharp critiques
        from, among others <a href=\"https://bjoern.brembs.net/2021/09/algorithmic-employment-decisions-in-academia/\">Bjorn
        Brembs</a>, <a href=\"https://zenodo.org/records/2656601\">Leslie Chan</a>,
        <a href=\"https://elephantinthelab.org/when-your-journal-reads-you/\">Renke
        Siems</a>, <a href=\"https://liberquarterly.eu/article/view/13561\">Lai Ma</a>,
        and <a href=\"https://www.sup.org/books/title/?id=33205\">Sarah Lamdan</a>
        have appeared at regular intervals.</p>\n<p>None of this has translated into
        much, not even awareness among the larger academic public. A coordinated campaign
        of advocacy and consciousness-raising should be paired with high-quality,
        in-depth studies of publisher data harvesting\u2014on the example of SPARC\u2019s
        recent <a href=\"https://sparcopen.org/news/2023/sparc-report-urges-action-to-address-concerns-with-sciencedirect-data-privacy-practices/\">ScienceDirect
        report</a>. Any effort like this should be built on the premise that another
        scholarly-publishing world is possible. Our prevailing joint-custody arrangement\u2014for-profit
        publishers and non-profit universities\u2014is a recent and reversible development.
        There are lots of good reasons to restore custody to the academy. The latest
        is to stop our work from fueling the publishers\u2019 AI profits.</p>\n<h2
        id=\"acknowledgments\">Acknowledgments</h2>\n<p>A peer-reviewed version of
        this blog post has been published in <a href=\"https://doi.org/10.18357/kula.291\">KULA</a>.</p>\n<hr
        class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li
        id=\"fn1\" class=\"footnote-item\"><p>The term itself is misleading, though
        now unavoidable. By AI (artificial intelligence), I am mostly referring to
        the bundle of techniques now routinely grouped under the \u201cmachine learning\u201d
        label. There is an irony in this linguistic capture. For decades after its
        coinage in the mid-1950s, \u201cartificial intelligence\u201d was used to
        designate a rival approach, grounded in rules and symbols. What most everyone
        now calls AI was, until about 30 years ago, excluded from the club. The story
        of how neural networks and other ML techniques won admission has not yet found
        its chronicler. What\u2019s clear is that a steep funding drop-off in the
        1980s (the so-called \u201cAI winter\u201d) made the once-excluded machine-learning
        rival\u2014its predictive successes on display over subsequent decades\u2014a
        very attractive aide to winning back the grant money. This essay is based
        on an invited talk delivered for Colgate University\u2019s <a href=\"https://jfinnell.colgate.domains/horizons/\">Horizons
        colloquium series</a> in October 2023. <a href=\"#fnref1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n","doi":"https://doi.org/10.54900/zg929-e9595","funding_references":null,"guid":"https://doi.org/10.54900/zg929-e9595","id":"1ac5fdea-3609-4ef6-8686-e1c36ab14fa0","image":"https://upstream.force11.org/content/images/2023/12/pexels-viktor-talashuk-2377295.jpg","indexed_at":1764757688,"language":"en","parent_doi":null,"published_at":1704207636,"reference":[],"registered_at":0,"relationships":[{"type":"IsPreprintOf","urls":["https://doi.org/10.18357/kula.291"]}],"rid":"4jymf-n5m83","subfield":null,"summary":"<em>\n
        The New York Times\n</em>\nushered in the New Year with a lawsuit against
        OpenAI and Microsoft. The paper covered the suit, fittingly, as a major business
        story.","tags":["Thought Pieces"],"title":"Large Language Publishing","topic":"13516","topic_score":0.21,"updated_at":1729244077,"url":"https://upstream.force11.org/large-language-publishing/","version":"v1"}

        '
    headers:
      content-encoding:
      - gzip
      content-type:
      - application/json
      date:
      - Thu, 04 Dec 2025 18:27:10 GMT
      fly-request-id:
      - 01KBN9WFV08M2VC9PN26CBY3A2-ams
      server:
      - Fly/340afcba (2025-12-03)
      transfer-encoding:
      - chunked
      vary:
      - Origin
      via:
      - 1.1 fly.io, 1.1 fly.io
    status:
      code: 200
      message: ''
version: 1
